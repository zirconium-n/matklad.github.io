<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<link href="https://matklad.github.io/feed.xml" rel="self" type="application/atom+xml"/>
<link href="https://matklad.github.io" rel="alternate" type="text/html"/>
<updated>2024-08-05T00:27:06.578Z</updated>
<id>https://matklad.github.io/feed.xml</id>
<title type="html">matklad</title>
<subtitle>Yet another programming blog by Alex Kladov aka matklad.</subtitle>
<author><name>Alex Kladov</name></author>

<entry>
<title type="text">Primitive Recursive Functions For A Working Programmer</title>
<link href="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html" rel="alternate" type="text/html" title="Primitive Recursive Functions For A Working Programmer" />
<published>2024-08-01T00:00:00+00:00</published>
<updated>2024-08-01T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/08/01/primitive-recursive-functions</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Programmers on the internet often use Turing-completeness terminology. Typically, not being
Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most
such discussions are misinformed --- that not being Turing complete doesn't actually mean what folks
want it to mean, and is instead a stand-in for a bunch of different practically useful properties,
which are mostly orthogonal to actual Turing completeness.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html"><![CDATA[
<h1><span>Primitive Recursive Functions For A Working Programmer</span> <time class="meta" datetime="2024-08-01">Aug 1, 2024</time></h1>
<p><span>Programmers on the internet often use </span>&ldquo;<span>Turing-completeness</span>&rdquo;<span> terminology. Typically, not being</span>
<span>Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most</span>
<span>such discussions are misinformed </span>&mdash;<span> that not being Turing complete doesn</span>&rsquo;<span>t actually mean what folks</span>
<span>want it to mean, and is instead a stand-in for a bunch of different practically useful properties,</span>
<span>which are mostly orthogonal to actual Turing completeness.</span></p>
<p><span>While I am generally descriptivist in nature and am ok with words loosing their </span><em><span>original</span></em><span> meaning</span>
<span>as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will</span>
<span>die on. It is a term from math, it has a very specific meaning, and you are not allowed to</span>
<span>re-purpose it for anything else, sorry!</span></p>
<p><span>I understand why this happens: to really understand what Turing completeness is and is not you need</span>
<span>to know one (simple!) theoretical result about so-called primitive recursive functions. And,</span>
<span>although this result is simple, I was only made aware of it in a fairly advanced course during my</span>
<span>masters. That</span>&rsquo;<span>s the CS education deficiency I want to rectify </span>&mdash;<span> you can</span>&rsquo;<span>t teach students the</span>
<span>halting problem without </span><em><span>also</span></em><span> teaching them about primitive recursion!</span></p>
<p><span>The post is going to be rather meaty, and will be split in three parts:</span></p>
<p><span>In Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going</span>
<span>to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions.</span>
<span>And then Part III will circle back to practical matters.</span></p>
<p><span>If math makes you slightly nauseas, you might to skip Part II. But maybe give it a try? The math</span>
<span>we</span>&rsquo;<span>ll need will be baby math from first principles, without reference to any advanced results.</span></p>
<section id="Part-I-TL-DR">

    <h2>
    <a href="#Part-I-TL-DR"><span>Part I: TL;DR</span> </a>
    </h2>
<p><span>Here</span>&rsquo;<span>s the key result </span>&mdash;<span> suppose you have a program in some Turing complete language, and you also</span>
<span>know that it</span>&rsquo;<span>s not too slow. Suppose it runs faster than</span>
<span class="display"><span>O(2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>).</span></span>
<span>That is, two to the power of two to the power of N, a very large number. In this case, you can</span>
<span>implement this algorithm in a non-Turing complete language.</span></p>
<p><span>Most practical problems fall into this </span>&ldquo;<span>faster than two to the two to the power of two</span>&rdquo;<span> space.</span>
<span>Hence it follows that you don</span>&rsquo;<span>t need full power of a Turing Machine to tackle them. Hence, a</span>
<span>language not being Turing complete doesn</span>&rsquo;<span>t in any way restrict you in practice, or gives you extra</span>
<span>powers to control the computation.</span></p>
<p><span>Or, to restate this: in practice, a program which doesn</span>&rsquo;<span>t terminate, and a program that needs a</span>
<span>billion billions steps to terminate are equivalent. Making something non-Turing complete by itself</span>
<span>doesn</span>&rsquo;<span>t help with the second problem in any way. And there</span>&rsquo;<span>s a trivial approach that solves the</span>
<span>first problem for any existing Turing-complete language </span>&mdash;<span> in the implementation, count the steps</span>
<span>and bail with an error after a billion.</span></p>
</section>
<section id="Part-II-Weird-Machines">

    <h2>
    <a href="#Part-II-Weird-Machines"><span>Part II: Weird Machines</span> </a>
    </h2>
<p><span>The actual theoretical result is quite a bit more general than that. It is (unsurprisingly)</span>
<span>recursive:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a function is computed by a Turing Machine, and the runtime of this machine is bounded by some</span>
<span>primitive recursive function of input, then the original function itself can be written as a</span>
<span>primitive recursive function.</span></p>
</blockquote>

</figure>
<p><span>It is expected that this sounds like gibberish at this point! So let</span>&rsquo;<span>s just go and prove this thing,</span>
<span>right here in this blog post! Will work up slowly towards this result. The plan is as follows:</span></p>
<ul>
<li>
<em><span>First</span></em><span>, to brush up notation, we</span>&rsquo;<span>ll define Finite State Machines.</span>
</li>
<li>
<em><span>Second</span></em><span>, we</span>&rsquo;<span>ll turn our humble Finite State Machine into the all-powerful Turing Machine (spoiler</span>
&mdash;<span> a Turing Machine is an FSM with a pair of stacks), and, as is customary, wave our hands about</span>
<span>the Universal Turing Machine.</span>
</li>
<li>
<em><span>Third</span></em><span>, we leave the cozy world of imperative programming and define primitive recursive</span>
<span>functions.</span>
</li>
<li>
<em><span>Finally</span></em><span>, we</span>&rsquo;<span>ll talk about the relative computational power of TMs and PRFs, including the teased</span>
<span>up result and more!</span>
</li>
</ul>
</section>
<section id="Finite-State-Machines">

    <h2>
    <a href="#Finite-State-Machines"><span>Finite State Machines</span> </a>
    </h2>
<p><dfn><span>Finite State Machines</span></dfn><span> are simple! An FSM takes a string as input, and returns a binary</span>
<span>answer, </span>&ldquo;<span>yes</span>&rdquo;<span> or </span>&ldquo;<span>no</span>&rdquo;<span>. Unsurprisingly an FSM has a finite number of states: Q0, Q1, </span>&hellip;<span>, Qn.</span>
<span>A subset of states are designated as </span>&ldquo;<span>yes</span>&rdquo;<span> states, the rest are </span>&ldquo;<span>no</span>&rdquo;<span> states. There</span>&rsquo;<span>s also one</span>
<span>specific starting state.</span></p>
<p><span>The behavior of the state machine is guided by a transition (step) function, </span><code>s</code><span>. This function</span>
<span>takes the current state of FSM, the next symbol of input, and returns a new state.</span></p>
<p><span>The semantics of FSM is determined by repeatably applying the single step function for all symbols of</span>
<span>the input, and noting whether the final state is a </span>&ldquo;<span>yes</span>&rdquo;<span> state or a </span>&ldquo;<span>no</span>&rdquo;<span> state.</span></p>
<p><span>Here</span>&rsquo;<span>s an FSM which accepts only strings of zeros and ones of even length:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:     { Q0, Q1 }</span>
<span class="line">Yes States: { Q0 }</span>
<span class="line">Start State:  Q0</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; State</span>
<span class="line">s Q0 0 = Q1</span>
<span class="line">s Q0 1 = Q1</span>
<span class="line">s Q1 0 = Q0</span>
<span class="line">s Q1 1 = Q0</span></code></pre>

</figure>
<p><span>This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length</span>
<span>(including an empty input).</span></p>
<p><span>What can FSMs do? As they give a binary answer, they are recognizers </span>&mdash;<span> they don</span>&rsquo;<span>t compute</span>
<span>functions, but rather just characterize certain sets of strings. A famous result is that the</span>
<span>expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can</span>
<span>write a regular expression for it, you could also do an FSM!</span></p>
<p><span>There are also certain things that state machines can</span>&rsquo;<span>t do. For example they can</span>&rsquo;<span>t enter an infinite</span>
<span>loop. Any FSM is linear in the input size and always terminates. But there are much more specific</span>
<span>sets of strings that couldn</span>&rsquo;<span>t be recognized by an FSM. Consider this set:</span></p>

<figure class="code-block">


<pre><code><span class="line">1</span>
<span class="line">010</span>
<span class="line">00100</span>
<span class="line">0001000</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>That is, an infinite set which contains </span>&lsquo;<span>1</span>&rsquo;<span>s surrounded by the equal number of </span>&lsquo;<span>0</span>&rsquo;<span>s on the both</span>
<span>sides. Let</span>&rsquo;<span>s prove that there isn</span>&rsquo;<span>t a state machine that recognizes this set!</span></p>
<p><span>As usually, suppose there </span><em><span>is</span></em><span> such a state machine. It has a certain number of states </span>&mdash;<span> maybe a</span>
<span>dozen, maybe a hundred, maybe a thousand, maybe even more. But let</span>&rsquo;<span>s say fewer than a million.</span>
<span>Then, let</span>&rsquo;<span>s take a string which looks like a million zeros, followed by one, followed by million</span>
<span>zeros. And let</span>&rsquo;<span>s observe our FSM eating this particular string.</span></p>
<p><span>First of all, because the string is in fact a one surrounded by the equal number of zeros on both</span>
<span>sides, the FSM ends up in a </span>&ldquo;<span>yes</span>&rdquo;<span> state. Moreover, because the length of the string is much greater</span>
<span>than the number of states in the state machine, the state machine necessary visits some state twice.</span>
<span>There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be</span>
<span>pretty long, but it</span>&rsquo;<span>s definitely shorter than the total number of states we have.</span></p>
<p><span>And now we can fool the state machine. Let</span>&rsquo;<span>s make it eat our string again, but this time, once it</span>
<span>completes the ABCDA cycle, we</span>&rsquo;<span>ll force it to traverse this cycle again. That is, the original cycle</span>
<span>corresponds to some portion of our giant string:</span></p>

<figure class="code-block">


<pre><code><span class="line">0000 0000000000000000000 00 .... 1 .... 00000</span>
<span class="line">     &lt;- cycle portion -&gt;</span></code></pre>

</figure>
<p><span>If we duplicate this portion, our string will no longer look like one surrounded by equal number of</span>
<span>twos, but the state machine will still in the </span>&ldquo;<span>yes</span>&rdquo;<span> state. Which is a contradiction that completes</span>
<span>the proof.</span></p>
</section>
<section id="Turing-Machine-Definition">

    <h2>
    <a href="#Turing-Machine-Definition"><span>Turing Machine: Definition</span> </a>
    </h2>
<p><span>A </span><dfn><span>Turing Machine</span></dfn><span> is only slightly more complex than an FSM. Like an FSM, a TM has a bunch of states</span>
<span>and a single-step transition function. While an FSM has an immutable input which is being feed to it</span>
<span>symbol by symbol, a TM operates with a mutable tape. The input gets written to the tape at the</span>
<span>start. At each step, a TM looks at the current symbol on the tape, changes its state according to a</span>
<span>transition function and, additionally:</span></p>
<ul>
<li>
<span>Replaces the current symbol with a new one (which might or might not be different).</span>
</li>
<li>
<span>Moves the reading head that points at the current symbol one position to the left or to the right.</span>
</li>
</ul>
<p><span>When a machine reaches a designated halt state, it stops, and whatever is written on the tape at</span>
<span>that moment is the result. That is, while FSMs are binary recognizers, TMs are functions. Keep in</span>
<span>mind that a TM does not necessary stop. It might be the case that a TM goes back and forth over the</span>
<span>tape, overwrites it, changes its internal state, but never quite gets to the final state.</span></p>
<p><span>Here</span>&rsquo;<span>s an example Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:  {A, B, C, H}</span>
<span class="line">Start State: A</span>
<span class="line">Final State: H</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; (State, Symbol, Left | Right)</span>
<span class="line">s A 0 = (B, 1, Right)</span>
<span class="line">s A 1 = (H, 1, Right)</span>
<span class="line">s B 0 = (C, 0, Right)</span>
<span class="line">s B 1 = (B, 1, Right)</span>
<span class="line">s C 0 = (C, 1, Left)</span>
<span class="line">s C 1 = (A, 1, Left)</span></code></pre>

</figure>
<p><span>If the configuration of the machine looks like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">000010100000</span>
<span class="line">     ^</span>
<span class="line">     B</span></code></pre>

</figure>
<p><span>Then we are in the </span><code>s B 0 = (C, 0, Right)</code><span> case, so we should change the state to C, replace 0 with</span>
<span>1, and move to the right:</span></p>

<figure class="code-block">


<pre><code><span class="line">000011100000</span>
<span class="line">      ^</span>
<span class="line">      C</span></code></pre>

</figure>
</section>
<section id="Turing-Machine-Programming">

    <h2>
    <a href="#Turing-Machine-Programming"><span>Turing Machine: Programming</span> </a>
    </h2>
<p><span>There is a bunch of fiddly details to Turing Machines!</span></p>
<p><span>The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a</span>
<span>problem: it might be hard to say where the input (or the output) ends! There are a couple of</span>
<span>technical solutions here. One is to say that there are three different symbols on the tape </span>&mdash;
<span>zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution</span>
<span>is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit</span>
<span>bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end</span>
<span>of input/output.</span></p>
<p><span>It</span>&rsquo;<span>s useful to think about how this byte-oriented TM could be implemented. We could have one large</span>
<span>state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And</span>
<span>then we</span>&rsquo;<span>ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in</span>
<span>state </span><code>S</code><span>. Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached</span>
<span>something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns</span>
<span>of Turing Machine programming </span>&mdash;<span> while your main memory is the tape, you can represent some</span>
<span>constant amount of memory directly in the states.</span></p>
<p><span>What we</span>&rsquo;<span>ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented</span>
<span>machine. So, we could think only in terms of big states operating on bytes, as we know the general</span>
<span>pattern for converting that to direct bit-twiddling.</span></p>
<p><span>With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will</span>
<span>be handy to the next observation:</span></p>
<p><span>You can</span>&rsquo;<span>t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn</span>&rsquo;<span>t</span>
<span>some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into</span>
<span>the machine. Transition function </span><em><span>is</span></em><span> the program.</span></p>
<p><span>But with some ingenuity we can regain our ability to write programs. Recall that we</span>&rsquo;<span>ve just learned</span>
<span>to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and</span>
<span>its input, and then feed that entire file as an input to an </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine which would</span>
<span>read the file, and act as if the machine specified there. Turing Machine can have an </span><code>eval</code>
<span>function.</span></p>
<p><span>Is such </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine possible? Yes! And it is not hard: if you spend couple of hours</span>
<span>programming Turing Machines by hand, you</span>&rsquo;<span>ll see that you pretty much can do anything </span>&mdash;<span> you can do</span>
<span>numbers, arithmetics, loops, control flow. It</span>&rsquo;<span>s just very very tedious.</span></p>
<p><span>So let</span>&rsquo;<span>s just declare that we</span>&rsquo;<span>ve actually coded up this Universal Turing Machine which simulates a</span>
<span>TM given to it as an input in a particular encoding.</span></p>
<p><span>This sort of construct also gives rise to Church-Turing thesis. We have a TM which can run other</span>
<span>TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork,</span>
<span>you could </span><em><span>also</span></em><span> implement a Python interpreter as a TM (you likely want to avoid doing that</span>
<span>directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter complied</span>
<span>to WASM). This sort of bidirectional interpretation shows that Python and TM have equivalent</span>
<span>computing power. Moreover, it</span>&rsquo;<span>s quite hard to come up with a reasonable computational device which</span>
<span>is more powerful than a Turing Machine.</span></p>
<p><span>There are computational devices that are strictly weaker than TMs though. Recall FSM. By this point,</span>
<span>it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a</span>
<span>Turing Machine can do as well. And it should be intuitively clear that TM is more powerful than an</span>
<span>FSM. FSM gets to use only a finite number of states. A TM has these same states, but it also posses</span>
<span>a tape which serves like an infinitely sized external memory.</span></p>
<p><span>Directly proving that you </span><em><span>can</span>&rsquo;<span>t</span></em><span> encode a Universal Turing Machine as an FSM sounds complicated,</span>
<span>so let</span>&rsquo;<span>s prove something simpler. Recall that we have established that there</span>&rsquo;<span>s no FSM that accepts</span>
<span>only ones surrounded by the equal number of zeros on both sides (because a sufficiently large word</span>
<span>of this form would necessary enter a cycle in a state machine, which could then be further pumped).</span>
<span>But it</span>&rsquo;<span>s actaully easy to write a Turing Machine that does this:</span></p>
<ul>
<li>
<span>Erase zero (at the left side of the tape)</span>
</li>
<li>
<span>Go to the right end of the tape</span>
</li>
<li>
<span>Erase zero</span>
</li>
<li>
<span>Go to the left side of the tape</span>
</li>
<li>
<span>Repeat</span>
</li>
<li>
<span>If what</span>&rsquo;<span>s left is a single </span><code>1</code><span> the answer is </span>&ldquo;<span>yes</span>&rdquo;<span>, otherwise it is a </span>&ldquo;<span>no</span>&rdquo;
</li>
</ul>
<p><span>We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it</span>
<span>necessary follows that there isn</span>&rsquo;<span>t an FSM that can simulate an arbitrary TM.</span></p>
<p><span>It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction</span>
<span>which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal</span>
<span>programming language. There isn</span>&rsquo;<span>t a standard data structure that behaves just like a tape.</span></p>
<p><span>One cool practical trick is to simulate the tape as a pair of stacks. Take this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Tape: A B C D E F G</span>
<span class="line">Head:     ^</span></code></pre>

</figure>
<p><span>And transform it to something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Left Stack:  [A, B, C]</span>
<span class="line">Right Stack: [G, F, E, D]</span></code></pre>

</figure>
<p><span>That is, everything to the left of the head is one stack, everything to the right, reversed, is the</span>
<span>other.  Here, moving the reading head left or right corresponds to popping a value off one stack and</span>
<span>pushing it onto another.</span></p>
<p><span>So, an equivalent-in-power definition would be to say that an TM is an FSM endowed with two</span>
<span>stacks.</span></p>
<p><span>This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be</span>
<span>called a pushdown automaton, and it would correspond to context-free languages. But that</span>&rsquo;<span>s beyond</span>
<span>the scope of this post!</span></p>
<p><span>There</span>&rsquo;<span>s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and</span>
<span>1. You could say that a stack is just a number! So, something like</span>
<code class="display">[1, 0, 1, 1]</code>
<span>will be</span>
<span class="display"><code>1 + 2 + 8 = 11</code><span>.</span></span>
<span>Looking at the top of the stack is </span><code>stack % 2</code><span>, removing item from the stack is </span><code>stack / 2</code><span> and</span>
<span>pushing x onto the stack is </span><code>stack * 2 + x</code><span>. We won</span>&rsquo;<span>t need this </span><em><span>right</span></em><span> now, so just hold onto ths</span>
<span>for a brief moment.</span></p>
</section>
<section id="Turing-Machine-Limits">

    <h2>
    <a href="#Turing-Machine-Limits"><span>Turing Machine: Limits</span> </a>
    </h2>
<p><span>Ok, so we have some idea about the lower bound for a power of a Turing Machine </span>&mdash;<span> FSMs are strictly</span>
<span>less expressive. What about the opposite direction? Is there some computation that a Turing Machine</span>
<span>is incapable of doing?</span></p>
<p><span>Yes! Let</span>&rsquo;<span>s construct a function which maps natural numbers to natural numbers, which can</span>&rsquo;<span>t be</span>
<span>implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That</span>
<span>means that we can actually enumerate all possible Turing Machines, and write then in a giant line,</span>
<span>from the most simple Turing Machine to more complex ones:</span></p>

<figure class="code-block">


<pre><code><span class="line">TM_0</span>
<span class="line">TM_1</span>
<span class="line">TM_2</span>
<span class="line">...</span>
<span class="line">TM_326</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>This is of course going to be an infinite list.</span></p>
<p><span>Now, let</span>&rsquo;<span>s see how TM0 behaves on input </span><code>0</code><span>: it either prints something, or doesn</span>&rsquo;<span>t terminate. Then,</span>
<span>note how TM1 behaves on input </span><code>1</code><span>, and generalizing, create function </span><code>f</code><span> that behaves as the nth TM</span>
<span>on input </span><code>n</code><span>. It might look something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(0) = 0</span>
<span class="line">f(1) = 111011</span>
<span class="line">f(2) = doesn't terminate</span>
<span class="line">f(3) = 0</span>
<span class="line">f(4) = 101</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s construct function </span><code>g</code><span> which is maximally diffed from </span><code>f</code><span>: where </span><code>f</code><span> gives </span><code>0</code><span>, </span><code>g</code><span> will</span>
<span>return </span><code>1</code><span>, and it will return </span><code>0</code><span> in all other cases:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(0) = 1</span>
<span class="line">g(1) = 0</span>
<span class="line">g(2) = 0</span>
<span class="line">g(3) = 1</span>
<span class="line">g(4) = 0</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>There isn</span>&rsquo;<span>t a Turing machine that computes </span><code>g</code><span>. For suppose there is. Then, it exists in our list of</span>
<span>all Turing Machines somewhere. Let</span>&rsquo;<span>s say it is TM1000064. So, if we feed </span><code>0</code><span> to it, it will return</span>
<code>g(0)</code><span>, which is </span><code>1</code><span>, which is different from </span><code>f(0)</code><span>. And the same holds for </span><code>1</code><span>, and </span><code>2</code><span>, and </span><code>3</code><span>.</span>
<span>But once we get to </span><code>g(1000064)</code><span>, we are in trouble, because, by the definition of </span><code>g</code><span>, </span><code>g(1000064)</code>
<span>is different from what is computed by TM1000064! So such a machine is impossible.</span></p>
<p><span>Those math savvy might express this more succinctly </span>&mdash;<span> there</span>&rsquo;<span>s a countably-infinite number of</span>
<span>Turing Machines, and an uncountably-infinite number of functions. So there </span><em><span>must</span></em><span> be some functions</span>
<span>which do not have a corresponding Turing Machine. It is the same proof </span>&mdash;<span> the diagonalization</span>
<span>argument is hiding in the claim that the set of all functions is an uncountable set.</span></p>
<p><span>But this is super weird and abstract. Let</span>&rsquo;<span>s rather come up with some very specific problem which</span>
<span>isn</span>&rsquo;<span>t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and</span>
<span>its input, determine if the machine halts on this input eventually.</span></p>
<p><span>As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have</span>
<span>equivalent computational power, I am going to try to solve this in Python:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># One million lines of readable, but somewhat</span></span>
<span class="line">    <span class="hl-comment"># unsettling and intimidating Python code.</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line">raw_input = <span class="hl-built_in">input</span>()</span>
<span class="line">[program_source_code, program_input] = parse(raw_input)</span>
<span class="line"><span class="hl-built_in">print</span>(<span class="hl-string">&quot;Yes&quot;</span> <span class="hl-keyword">if</span> halts(program_source_code, program_input) <span class="hl-keyword">else</span> <span class="hl-string">&quot;No&quot;</span>)</span></code></pre>

</figure>
<p><span>Now, I will do a weird thing and start asking whether a program termintates, if it is fed its own</span>
<span>source code, in a reverse-quine of sorts:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span></code></pre>

</figure>
<p><span>and finally I construct this weird beast of a program:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># ...</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">weird</span>(<span class="hl-params">program_input</span>):</span>
<span class="line">    <span class="hl-keyword">if</span> halts_on_self(program_input):</span>
<span class="line">        <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">            <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line">weird(<span class="hl-built_in">input</span>())</span></code></pre>

</figure>
<p><span>To make this even worse, I</span>&rsquo;<span>ll feed the text of this </span><code>weird</code><span> program to itself. Does it terminate</span>
<span>with this input? Well, if it terminates, and if our </span><code>halts</code><span> function is implemented correctly, then</span>
<span>the </span><code>halts_on_self(program_input)</code><span> invocation above returns </span><code>True</code><span>. But then we enter the infinite</span>
<span>loop and don</span>&rsquo;<span>t actually terminate.</span></p>
<p><span>Hence, it must be the case that </span><code>weird</code><span> does not terminate when self-applied. But then</span>
<code>halts_on_self</code><span> returns </span><code>False</code><span>, and it should terminate. So we get a contradiction both ways. Which</span>
<span>necessary means that either our </span><code>halts</code><span> sometimes returns a straight-up incorrect answer, or that it</span>
<span>sometimes does not terminate.</span></p>
<p><span>So this is the flip side of Turing Machine</span>&rsquo;<span>s power </span>&mdash;<span> it is so powerful that it becomes impossible</span>
<span>to tell whether it</span>&rsquo;<span>ll terminate or not!</span></p>
<p><span>It actually gets much worse, because this result can be generalized to an unreasonable degree!</span>
<span>In general, there</span>&rsquo;<span>s very little we can say about arbitrary programs.</span></p>
<p><span>We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they</span>
<span>are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program.</span>
<span>It would be much more interesting to check some refactoring-invariant properties, which hold when</span>
<span>you change the text of the program, but leave the behavior intact. Indeed, </span>&ldquo;<span>does this change</span>
<span>preserve behavior?</span>&rdquo;<span> would be one very useful property to check!</span></p>
<p><span>So let</span>&rsquo;<span>s define two TMs to be equivalent, if they have identical behavior. That is, for each</span>
<span>specific input, either both machines don</span>&rsquo;<span>t terminate, or they both halt, and give identical results.</span></p>
<p><span>Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold)</span>
<span>for the entire classes of equivalence of TMs.</span></p>
<p><span>And a somewhat depressing result here is that there are no non-trivial refactoring-invariant</span>
<span>properties that you can algorithmically check.</span></p>
<p><span>Suppose we have some magic TM, called P, which checks such a property. Let</span>&rsquo;<span>s show that, using P, we can</span>
<span>solve the problem we know we can not solve </span>&mdash;<span> the halting problem.</span></p>
<p><span>Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might</span>
<span>not hold for it. But, because P is not-trivial (it holds for some machines and doesn</span>&rsquo;<span>t hold for some</span>
<span>machines), there</span>&rsquo;<span>s some different machine M2 which differs from M1 with respect to P. That is,</span>
<code>P(M1) xor P(M2)</code><span> holds.</span></p>
<p><span>Let</span>&rsquo;<span>s use these M1 and M2 to figure out where a given machine M halts on input I. Using Universal</span>
<span>Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then</span>
<span>erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is</span>
<span>behaviorally-equivalent to M2. If M doesn</span>&rsquo;<span>t halt on I, then the result is equivalent to the infinite</span>
<span>loop program, M1. Or, in pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M1</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">        <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M2</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-comment"># We don&#x27;t actually know what&#x27;s here</span></span>
<span class="line">    <span class="hl-comment"># but we know that such a machine exists.</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">assert</span>(P(M1) != P(M2))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">M, I</span>):</span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">M12</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">        M(I) <span class="hl-comment"># might or might not halt</span></span>
<span class="line">        <span class="hl-keyword">return</span> M2(<span class="hl-built_in">input</span>)</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">return</span> P(M12) == P(M2)</span></code></pre>

</figure>
<p><span>This is pretty bad and depressing </span>&mdash;<span> can</span>&rsquo;<span>t learn anything meaningful about an arbitrary Turing</span>
<span>Machine! So let</span>&rsquo;<span>s finally get to the actual topic of today</span>&rsquo;<span>s post:</span></p>
</section>
<section id="Primitive-Recursive-Functions">

    <h2>
    <a href="#Primitive-Recursive-Functions"><span>Primitive Recursive Functions</span> </a>
    </h2>
<p><span>This is going to be another computational device, like FSMs and TMs. Like FSM, it</span>&rsquo;<span>s going to be a</span>
<span>nice, always terminating, non-Turing complete device. But it would turn out to have quite a bit of</span>
<span>power of a full Turing Machine!</span></p>
<p><span>However, unlike both TMs and FSMs, </span><dfn><span>Primitive Recursive Functions</span></dfn><span> are defined directly as</span>
<span>functions which take a tuple of natural numbers and return a natural number. The two simplest ones</span>
<span>are </span><code>zero</code><span> (that is, zero-arity function that returns </span><code>0</code><span>) and </span><code>succ</code><span> </span>&mdash;<span> an unary function that</span>
<span>just adds 1. Everything else is going to get constructed out of these two:</span></p>

<figure class="code-block">


<pre><code><span class="line">zero = 0</span>
<span class="line">succ(x) = x + 1</span></code></pre>

</figure>
<p><span>One way we are allowed to combine these functions is by composition. So we can get all the constants</span>
<span>right of the bat:</span></p>

<figure class="code-block">


<pre><code><span class="line">succ(zero) = 1</span>
<span class="line">succ(succ(zero)) = 2</span>
<span class="line">succ(succ(succ(zero))) = 3</span></code></pre>

</figure>
<p><span>We aren</span>&rsquo;<span>t going to get allowed to use general recursion (because it can trivially non-terminate),</span>
<span>but we do get to use a restricted form of C-style loop. It is a bit fiddly to defile formally! The</span>
<span>overall shape is </span><span class="display"><code>LOOP(init, f, n)</code><span>.</span></span></p>
<p><span>Here, </span><code>init</code><span> and </span><code>n</code><span> are numbers </span>&mdash;<span> initial value of accumulator and the total number of</span>
<span>iterations. The </span><code>f</code><span> is an unary function that specifies the loop body </span>&ndash;<span> it takes the current value</span>
<span>of the accumulator and returns the new value. So</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(init, f, 0) = init</span>
<span class="line">LOOP(init, f, 1) = f(init)</span>
<span class="line">LOOP(init, f, 2) = f(f(init))</span>
<span class="line">LOOP(init, f, 3) = f(f(f(init)))</span></code></pre>

</figure>
<p><span>While this is </span><em><span>similar</span></em><span> to a C-style loop, the crucial difference here is that the total number of</span>
<span>iterations </span><code>n</code><span> is fixed up-front. There</span>&rsquo;<span>s no way to mutate the loop counter in the loop body.</span></p>
<p><span>This allows us to define addition:</span></p>

<figure class="code-block">


<pre><code><span class="line">add(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>Multiplication is trickier. Conceptually, to multiply </span><code>x</code><span> and </span><code>y</code><span>, we want to </span><code>LOOP</code><span> from zero, and</span>
<span>repeat </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> </span><code>y</code><span> times. The problem here is that we can</span>&rsquo;<span>t write </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> function yet</span></p>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work, add is a binary function!</span>
<span class="line">mul(x, y) = LOOP(0, add, y)</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work either, no x in scope!</span>
<span class="line">add_x v = add(x, v)</span>
<span class="line">mul(x, y) = LOOP(0, add_x, y)</span></code></pre>

</figure>
<p><span>One way around this is to defile </span><code>LOOP</code><span> as a family of operators, which can pass extra arguments to</span>
<span>the iteration function:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP0(init, f, 2) = f(f(init))</span>
<span class="line">LOOP1(c1, init, f, 2) = f(c1, f(c1, init))</span>
<span class="line">LOOP2(c1, c2, init, f, 2) = f(c1, c2, f(c1, c2, init))</span></code></pre>

</figure>
<p><span>That is, </span><code>LOOP_N</code><span> takes extra </span><code>n</code><span> arguments, and passes them through to any invocation of the body</span>
<span>function. To express this idea a little bit more succinctly, let</span>&rsquo;<span>s just allow to partially  apply</span>
<span>the second argument of </span><code>LOOP</code><span>. That is:</span></p>
<ul>
<li>
<span>All our functions are going to be first order. All arguments are numbers, the result is a number.</span>
<span>There aren</span>&rsquo;<span>t high order functions, there aren</span>&rsquo;<span>t closures.</span>
</li>
<li>
<span>The </span><code>LOOP</code><span> is not a function in our language </span>&mdash;<span> its a builtin operator, a keyword. So, for</span>
<span>convenience, we allow to pass partially applied functions to it. But semantically this is</span>
<span>equivalent to just passing in extra argumennts on each iteration.</span>
</li>
</ul>
<p><span>Which finally allows us to write</span></p>

<figure class="code-block">


<pre><code><span class="line">mul(x, y) = LOOP(0, add x, y)</span></code></pre>

</figure>
<p><span>Ok, so that</span>&rsquo;<span>s progress </span>&mdash;<span> we made something as complicated as multiplication, and we still are in</span>
<span>the guaranteed-to-terminate land. Because each loop has a fixed number of iteration, everything</span>
<span>eventually finishes.</span></p>
<p><span>We can go on and define x</span><sup><span>y</span></sup><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow(x, y) = LOOP(1, mul x, y)</span></code></pre>

</figure>
<p><span>And this in turn allows to define a couple of concerning fast growing functions:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow_2(n) = pow(2, n)</span>
<span class="line">pow_2_2(n) = pow_2(pow_2(n))</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s fun, but to do some programming, we</span>&rsquo;<span>ll need an </span><code>if</code><span>. We</span>&rsquo;<span>ll get to it, but first we</span>&rsquo;<span>ll need</span>
<span>some boolean operations. We can encode </span><code>false</code><span> as </span><code>0</code><span> and </span><code>true</code><span> as </span><code>1</code><span>. Then</span></p>

<figure class="code-block">


<pre><code><span class="line">and(x, y) = mul(x, y)</span></code></pre>

</figure>
<p><span>But </span><code>or</code><span> creates a problem: we</span>&rsquo;<span>ll need a subtraction.</span></p>

<figure class="code-block">


<pre><code><span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Defining </span><code>sub</code><span> is tricky, due to two problems:</span></p>
<p><span>First, we only have natural numbers, no negatives. This one is easy to solve </span>&mdash;<span> we</span>&rsquo;<span>ll just define</span>
<span>subtraction to saturate.</span></p>
<p><span>The second problem is more severe </span>&mdash;<span> I think we actually can</span>&rsquo;<span>t express subtraction given the set of</span>
<span>allowable operations so far. That is because all our operations are monotonic </span>&mdash;<span> the result is</span>
<span>never less than the arguments. One way to solve this problem is to defile the LOOP in such a way</span>
<span>that the body function also gets passed a second argument </span>&mdash;<span> the current iteration. So, if you</span>
<span>iterate up to </span><code>n</code><span>, the last iteration will observe </span><code>n - 1</code><span>, and that would be the non-monotonic</span>
<span>operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just</span>
<span>add a </span><code>pred</code><span> function to the basis, and use that to add loop counters to our iterations.</span></p>

<figure class="code-block">


<pre><code><span class="line">pred(0) = 0 # saturate</span>
<span class="line">pred(1) = 0</span>
<span class="line">pred(2) = 1</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now we can say:</span></p>

<figure class="code-block">


<pre><code><span class="line">sub(x, y) = LOOP(x, pred, y)</span>
<span class="line"></span>
<span class="line">and(x, y) = mul(x, y)</span>
<span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y)</span>
<span class="line">)</span>
<span class="line">not(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line">if(cond, a, b) = add(</span>
<span class="line">  mul(a, cond),</span>
<span class="line">  mul(b, not(cond)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we can do a bunch of comparison operators:</span></p>

<figure class="code-block">


<pre><code><span class="line">is_zero(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line"># x &gt;= y</span>
<span class="line">ge(x, y) = is_zero(sub(y, x))</span>
<span class="line"></span>
<span class="line"># x == y</span>
<span class="line">eq(x, y) = and(ge(x, y), ge(y, x))</span>
<span class="line"></span>
<span class="line"># x &gt; y</span>
<span class="line">gt(x, y) = and(ge(x, y), not(eq(x, y)))</span>
<span class="line"></span>
<span class="line"># x &lt; y</span>
<span class="line">lt(x, y) = gt(y, x)</span></code></pre>

</figure>
<p><span>With that we could implement modulus. To compute </span><code>x % m</code><span> we will start with </span><code>x</code><span>, and will be</span>
<span>subtracting </span><code>m</code><span> until we get a number smaller than </span><code>m</code><span>. We</span>&rsquo;<span>ll need at most </span><code>x</code><span> iterations for that.</span></p>
<p><span>In pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">mod</span>(<span class="hl-params">x, m</span>):</span>
<span class="line">  current = x</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">for</span> _ <span class="hl-keyword">in</span> <span class="hl-number">0.</span>.x:</span>
<span class="line">    <span class="hl-keyword">if</span> current &lt; m:</span>
<span class="line">      current = current</span>
<span class="line">    <span class="hl-keyword">else</span>:</span>
<span class="line">      current = current - m</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">return</span> current</span></code></pre>

</figure>
<p><span>And as a bona fide PRF:</span></p>

<figure class="code-block">


<pre><code><span class="line">mod_iter(m, x) = if(</span>
<span class="line">  lt(x, m),</span>
<span class="line">  x,        # then</span>
<span class="line">  sub(x, m) # else</span>
<span class="line">)</span>
<span class="line">mod(x, m) = LOOP(x, mod_iter m, x)</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s a curious structure </span>&mdash;<span> rather than computing the modulo directly, we essentially search for</span>
<span>it using trial and error, and relying on the fact that the search has a clear upper bound.</span></p>
<p><span>Division can be done similarly: to divide x by y, start with 0, and then repeatedly add one to the</span>
<span>accumulator until the product of accumulator and y exceeds x:</span></p>

<figure class="code-block">


<pre><code><span class="line">div_iter x y acc = if(</span>
<span class="line">  le(mul(succ(acc), y), y),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc        # else</span>
<span class="line">)</span>
<span class="line">div(x, y) = LOOP(0, div_iter x y, x)</span></code></pre>

</figure>
<p><span>This really starts looking like programming! One thing we are currently missing are data structures.</span>
<span>While our functions take multiple arguments, they only return one number. But it</span>&rsquo;<span>s easy enough to</span>
<span>pack two numbers into one: to represent an </span><code>(a, b)</code><span> pair, we</span>&rsquo;<span>ll use 2</span><sup><span>a</span></sup><span> 3</span><sup><span>b</span></sup><span> number:</span></p>

<figure class="code-block">


<pre><code><span class="line">mk_pair(a, b) = mul(pow(2, a), pow(3, b))</span></code></pre>

</figure>
<p><span>To deconstruct such a pair into its first and second components, we need to find the maximum power</span>
<span>of 2 or 3 that divides our number. Which is exactly the same shape we used to implement </span><code>div</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">max_factor_iter p m acc = if(</span>
<span class="line">  is_zero(mod(p, pow(m, succ(acc)))),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc,       # else</span>
<span class="line">)</span>
<span class="line">max_factor(p, m) = LOOP(0, max_factor_iter p m, p)</span>
<span class="line"></span>
<span class="line">fst(p) = max_factor(p, 2)</span>
<span class="line">snd(p) = max_factor(p, 3)</span></code></pre>

</figure>
<p><span>Here again we use the fact that the maximal power of two that divides </span><code>p</code><span> is not larger than </span><code>p</code>
<span>itself, so we can over-estimate the number of iterations we</span>&rsquo;<span>ll need as </span><code>p</code><span>.</span></p>
<p><span>Using this pair construction, we can finally add a loop counter to our </span><code>LOOP</code><span> construct. To track</span>
<span>the counter, we pack it as a pair with the accumulator:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(mk_pair(init, 0), f, n)</span></code></pre>

</figure>
<p><span>And then inside f, we first unpack that pair into accumulator and counter, pass them to actual loop</span>
<span>iteration, and then pack the result again, incrementing the counter:</span></p>

<figure class="code-block">


<pre><code><span class="line">f acc = mk_pair(</span>
<span class="line">  g(fst(acc), snd(acc)),</span>
<span class="line">  succ(snd(acc)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Ok, so we have achieved something remarkable: while we are writing terminating-by-construction</span>
<span>programs, which are definitely not Turing complete, we have constructed basic programming staples,</span>
<span>like boolean logic and data structures, and we have also build some rather complicated mathematical</span>
<span>functions, like </span><span class="display"><span>2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>.</span></span></p>
<p><span>We could try to further enrich our little primitive recursive kingdom by adding more and more</span>
<span>functions on the ad hoc basis, but let</span>&rsquo;<span>s try to be really ambitious and go for the main prize </span>&mdash;
<span>simulating Turing Machines.</span></p>
<p><span>We know that we will fail: Turing machine can enter the infinite loop, but PRF necessary terminates.</span>
<span>That means, that, if PRF were able to simulate an arbitrary TM, it would have to say after certain</span>
<span>finite amount of steps that </span>&ldquo;<span>this TM doesn</span>&rsquo;<span>t terminate</span>&rdquo;<span>.  And, while we didn</span>&rsquo;<span>t do this, it</span>&rsquo;<span>s easy to</span>
<span>see that you </span><em><span>could</span></em><span> simulate the other way around and implement PRFs in a TM. But that would give</span>
<span>us an TM algorithm to decide if an arbitrary TM halts, which we know doesn</span>&rsquo;<span>t exist.</span></p>
<p><span>So, this is hopeless! But we might still be able to learn something from failing.</span></p>
<p><span>Ok! So let</span>&rsquo;<span>s start with a configuration of a TM which we somehow need to encode into a single</span>
<span>number. First, we need the state variable proper (Q0, Q1, etc), which seems easy enough to represent</span>
<span>with a number. Then, we need a tape and a position of the reading head. Recall how we used a pair of</span>
<span>stacks to represent exactly the tape and the position. And recall that we can look at a stack of</span>
<span>zeros and ones as a number in binary form, where push and pop operations are implemented using </span><code>%</code><span>,</span>
<code>*</code><span>, and </span><code>/</code><span> </span>&mdash;<span> exactly the operations we already can do. So, our configuration is just three</span>
<span>numbers: </span><span class="display"><code>(S, stack1, stack2)</code><span>.</span></span></p>
<p><span>And, using 2</span><sup><span>a</span></sup><span>3</span><sup><span>b</span></sup><span>5</span><sup><span>c</span></sup><span> trick, we can pack this triple just into a single number. But that means we</span>
<span>could directly encode a single step of a Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">single_step(config) = if(</span>
<span class="line">  # if the state is Q0 ...</span>
<span class="line">  eq(fst(config), 0)</span>
<span class="line"></span>
<span class="line">  # and the symbol at the top of left stack is 0</span>
<span class="line">  if(is_zero(mod(snd(config), 2))</span>
<span class="line">    mk_triple(</span>
<span class="line">      1,                    # move to state Q1</span>
<span class="line">      div(snd(config), 2),  # pop value from the left stack</span>
<span class="line">      mul(trd(config), 2),  # push zero onto the right stack</span>
<span class="line">    ),</span>
<span class="line">    ... # Handle symbol 1 in state Q1</span>
<span class="line">  )</span>
<span class="line">  # if the state is Q1 ...</span>
<span class="line">  if(eq(fst(config), 1)</span>
<span class="line">    ...</span>
<span class="line">  )</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we could plug that into our </span><code>LOOP</code><span> to simulate Turing Machine running for N steps:</span></p>

<figure class="code-block">


<pre><code><span class="line">n_steps initial_config n =</span>
<span class="line">  LOOP(initial_config, single_step, n)</span></code></pre>

</figure>
<p><span>The catch of course that we can</span>&rsquo;<span>t know the </span><code>N</code><span> that</span>&rsquo;<span>s going to be enough. But we can have a very</span>
<span>good guess! We could do something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">hopefully_enough_steps initial_config =</span>
<span class="line">  LOOP(initial_config, single_step, pow_2_2(initial_config))</span></code></pre>

</figure>
<p><span>That is, run for some large tower of exponents of the initial state. Which would be plenty for</span>
<span>normal algorithms, which are usually 2</span><sup><span>N</span></sup><span> at worst!</span></p>
<p><span>Or, generalizing:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a TM has a runtime which is bounded by some primitive-recursive function, then the entire</span>
<span>TM can be replaced with a PRF. Be advised that PRFs can grow </span><em><span>really</span></em><span> fast.</span></p>
</blockquote>

</figure>
<p><span>Which is the headline result we have set out to prove!</span></p>
</section>
<section id="Primitive-Recursive-Functions-Limit">

    <h2>
    <a href="#Primitive-Recursive-Functions-Limit"><span>Primitive Recursive Functions: Limit</span> </a>
    </h2>
<p><span>It might seem that non-termination is the only principle obstacle. That anything that terminates at</span>
<span>all has to be implementable as a PRF. Alas, that</span>&rsquo;<span>s not so. Let</span>&rsquo;<span>s go and construct a function that is</span>
<span>surmountable by a TM, but is out of reach of PRFs.</span></p>
<p><span>We will combine the ideas of the impossibility proofs for FSMs (noting that if a function is</span>
<span>computed by some machine, that machine has a specific finite size) and TMs (diagonalization).</span></p>
<p><span>So, suppose we have some function </span><code>f</code><span> that can</span>&rsquo;<span>t be computed by a PRF. How would we go about proving</span>
<span>that? Well, we</span>&rsquo;<span>d start with </span>&ldquo;<span>suppose that we have a PRF P that computes </span><code>f</code>&rdquo;<span>. And then we could</span>
<span>notice that P would save some finite size. If you look at it abstractly, the P is its syntax tree,</span>
<span>with lots of </span><code>LOOP</code><span> construct, but it always boils down to some </span><code>succ</code><span>s and </span><code>zero</code><span>s at the leaves.</span>
<span>Let</span>&rsquo;<span>s say that the depth of P is </span><code>d</code><span>.</span></p>
<p><span>And, actually, if you look at it, there are only finite number of PRFs with depth at most </span><code>d</code><span>. Some</span>
<span>of them describe pretty fast growing functions. But probably there</span>&rsquo;<span>s a limit to how fast a function</span>
<span>can grow, given that it is computed by a PRF of size </span><code>d</code><span>. Or, to use a concrete example: we have</span>
<span>constructed a PRF of depth 5 that computes two to the power of two to the power of N. Probably if we</span>
<span>were smarter, we could have squeezed a couple more levels into that tower of exponents. But</span>
<span>intuitively it seems that if you build a tower of, say, 10 exponents, that would grow faster than</span>
<em><span>any</span></em><span> PRF of depth </span><code>5</code><span>. And that this generalizes </span>&mdash;<span> for any fixed depth, there</span>&rsquo;<span>s a high-enough</span>
<span>tower of exponents that grows faster than any PRF with that depth.</span></p>
<p><span>So we could conceivably build an </span><code>f</code><span> that defeats our </span><code>d</code><span>-deep P. But that</span>&rsquo;<span>s not quite a victory</span>
<span>yet: maybe that </span><code>f</code><span> is feasible for </span><code>d+2</code><span>-deep PRF! So here we</span>&rsquo;<span>ll additionally apply</span>
<span>diagonalization: for each depth, we</span>&rsquo;<span>ll build it</span>&rsquo;<span>s own depth-specific nemesis </span><code>f_d</code><span>. And then we</span>&rsquo;<span>ll</span>
<span>define our overall function as</span></p>

<figure class="code-block">


<pre><code><span class="line">a(n) = f_n(n)</span></code></pre>

</figure>
<p><span>So, for </span><code>n</code><span> large enough it</span>&rsquo;<span>ll grow faster than a PRF with any fixed depth.</span></p>
<p><span>So that</span>&rsquo;<span>s the general plan, the rest of the own is basically just calculating the upper bound on the</span>
<span>growth of a PRF of depth </span><code>d</code><span>.</span></p>
<p><span>One technical difficulty here is that PRFs tend to have different artities:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y)</span>
<span class="line">g(x, y, z, t)</span>
<span class="line">h(x)</span></code></pre>

</figure>
<p><span>Ideally, we</span>&rsquo;<span>d use just one upper bound of them all. So we</span>&rsquo;<span>ll be looking for an upper bound of the</span>
<span>following form:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, t) &lt;= A_d(max(x, y, z, t))</span></code></pre>

</figure>
<p><span>That is:</span></p>
<ul>
<li>
<span>Compute the depth of </span><code>f</code><span>, </span><code>d</code><span>.</span>
</li>
<li>
<span>Compute the largest of its arguments.</span>
</li>
<li>
<span>And plug that into unary function for depth </span><code>d</code><span>.</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with </span><code>d=1</code><span>. We have only primitive functions on this level, </span><code>succ</code><span>, </span><code>zero</code><span>, and </span><code>pred</code><span>,</span>
<span>so we could say that</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s handle arbitrary other depth </span><code>d + 1</code><span>. In that case, our function is non-primitive, so at</span>
<span>the root of the syntax tree we have either a composition or a </span><code>LOOP</code><span>.</span></p>
<p><span>Composition would look like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, ...) = g(</span>
<span class="line">  h1(x, y, z, ...),</span>
<span class="line">  h2(x, y, z, ...),</span>
<span class="line">  h3(x, y, z, ...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>where </span><code>g</code><span> and </span><code>h_n</code><span> are </span><code>d</code><span> deep and the resulting </span><code>f</code><span> is </span><code>d+1</code><span> deeep. We can immediately estimate</span>
<span>the </span><code>h_n</code><span> then:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= g(</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  ...</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>In this somewhat loose notation, </span><code>args...</code><span> stands for a tuple of arguments, and </span><code>maxarg</code><span> stands for</span>
<span>the largest one.</span></p>
<p><span>And then we could use the same estimate for </span><code>g</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>This is super high-order, so let</span>&rsquo;<span>s do a concrete example for depth-2 two-argument function which</span>
<span>starts with a composition:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) &lt;= A_1(A_1(max(x, y)))</span>
<span class="line">         = A_1(max(x, y) + 1)</span>
<span class="line">         = max(x, y) + 2</span></code></pre>

</figure>
<p><span>This sounds legit: if we don</span>&rsquo;<span>t use LOOP, then </span><code>f(x, y)</code><span> is either </span><code>succ(succ(x))</code><span> or </span><code>succ(succ(y))</code>
<span>so </span><code>max(x, y) + 2</code><span> indeed is the bound!</span></p>
<p><span>Ok, now the fun case! If the top-level node is a </span><code>LOOP</code><span>, then we have</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  g(args...),</span>
<span class="line">  h(args...),</span>
<span class="line">  t(args...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>This sounds complicated to estimate, especially due to that last </span><code>t(args...)</code><span> argument, which is the</span>
<span>number of iterations. So we</span>&rsquo;<span>ll be cowards and </span><em><span>won</span>&rsquo;<span>t</span></em><span> actually try to estimate this case. Instead,</span>
<span>we will require that our PRF is written in a simplified form, where the first and the last arguments</span>
<span>to </span><code>LOOP</code><span> are simple.</span></p>
<p><span>So, if your PRF looks like</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x + y, mul, pow2(x))</span></code></pre>

</figure>
<p><span>you are required to re-write it first as</span></p>

<figure class="code-block">


<pre><code><span class="line">helper(u, v) = LOOP(u, mul, v)</span>
<span class="line">f(x, y) = helper(x + y, pow2(x))</span></code></pre>

</figure>
<p><span>So now we only have to deal with this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  arg,</span>
<span class="line">  g(args...),</span>
<span class="line">  arg,</span>
<span class="line">)</span></code></pre>

</figure>
<p><code>f</code><span> has depth </span><code>d+1</code><span>, </span><code>g</code><span> has depth </span><code>d</code><span>.</span></p>
<p><span>On the first iteration, we</span>&rsquo;<span>ll call </span><code>g(args..., arg)</code><span>, which we can estimate as </span><code>A_d(maxarg)</code><span>. That</span>
<span>is, </span><code>g</code><span> does get an </span><em><span>extra</span></em><span> argument, but it is one of the original arguments of </span><code>f</code><span>, and we are</span>
<span>looking at the maximum argument anyway, so it doesn</span>&rsquo;<span>t matter.</span></p>
<p><span>On the second iteration, we are going to call</span>
<code class="display">g(args..., prev_iteration)</code>
<span>which we can estimate as</span>
<span class="display"><code>A_d(max(maxarg, prev_iteration))</code><span>.</span></span></p>
<p><span>Now we plug our estimation for the first iteration:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(args..., prev_iteration)</span>
<span class="line">  &lt;= A_d(max(maxarg, prev_iteration))</span>
<span class="line">  &lt;= A_d(max(maxarg, A_d(maxarg)))</span>
<span class="line">  =  A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>That is, the estimate for the first iteration is </span><code>A_d(maxarg)</code><span>. The estimation for the second</span>
<span>iteration adds one more layer: </span><code>A_d(A_d(maxarg))</code><span>. For the third iteration we</span>&rsquo;<span>ll get</span>
<span class="display"><code>A_d(A_d(A_d(maxarg)))</code><span>.</span></span></p>
<p><span>So the overall thing is going to be smaller than </span><code>A_d</code><span> iteratively applied to itself some number of</span>
<span>times, where </span>&ldquo;<span>some number</span>&rdquo;<span> is one of the </span><code>f</code><span> original arguments. But no harm</span>&rsquo;<span>s done if we iterate up</span>
<span>to </span><code>maxarg</code><span>.</span></p>
<p><span>As a sanity check, the worst depth-2 function constructed with iteration is probably</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>which is </span><code>x + y</code><span>. And our estimate gives </span><code>x + 1</code><span> applied </span><code>maxarg</code><span> times to </span><code>maxarg</code><span>, which is </span><code>2 *
maxarg</code><span>, which is indeed the correct upper bound!</span></p>
<p><span>Combining everything together, we have:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span>
<span class="line"></span>
<span class="line">f(args...) &lt;= max(</span>
<span class="line">  A_d(A_d(maxarg)),               # composition case</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))), # LOOP case,</span>
<span class="line">   &lt;-    maxarg A's         -&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>That </span><code>max</code><span> there is significant </span>&mdash;<span> although it seems like the second line, with </span><code>maxarg</code>
<span>applications, is, always going to be longer, </span><code>maxarg</code><span>, in fact, could be as small as zero. But we</span>
<span>can take </span><code>maxarg + 2</code><span> repetitions to fix this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;=</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))),</span>
<span class="line">  &lt;-    maxarg + 2 A's         -&gt;</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s just define </span><code>A_{d+1}(x)</code><span> to make that inequality work:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_{d+1}(x) = A_d(A_d( .... A_d(x)))</span>
<span class="line">            &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>Unpacking:</span></p>
<p><span>We define a family of unary functions </span><code>A_d</code><span>, such that each </span><code>A_d</code><span> </span>&ldquo;<span>grows faster</span>&rdquo;<span> than any n-ary PRF</span>
<span>of depth </span><code>d</code><span>. If </span><code>f</code><span> is a ternary PRF of depth 3, then </span><span class="display"><code>f(1, 92, 10) &lt;= A_3(92)</code><span>.</span></span></p>
<p><span>To evaluate </span><code>A_d</code><span> at point </span><code>x</code><span>, we use the following recursive procedure:</span></p>
<ul>
<li>
<span>If </span><code>d</code><span> is </span><code>1</code><span>, return </span><code>x + 1</code><span>.</span>
</li>
<li>
<span>Otherwise, evaluate </span><code>A_{d-1}</code><span> at point </span><code>x</code><span> to get, say, </span><code>v</code><span>. Then evaluate </span><code>A_{d-1}</code><span> again at</span>
<span>point </span><code>v</code><span> this time, yielding </span><code>u</code><span>. Then compute </span><code>A_{d-1}(u)</code><span>. Overall, repeat this process </span><code>x+2</code>
<span>times, and return the final number.</span>
</li>
</ul>
<p><span>We can simplify this a bit if we stop treating </span><code>d</code><span> as a kind of function </span><em><span>index</span></em><span>, and instead say</span>
<span>that our </span><code>A</code><span> is just a function of two arguments. Then we have the following equations:</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, x) = A(d, A(d, A(d, ..., A(d, x))))</span>
<span class="line">                &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>The last equation can re-formatted as</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d, A(d, ..., A(d, x))),</span>
<span class="line">  &lt;- x + 1 A_d's in total-&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And for non-zero x that is just</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d + 1, x - 1),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>So we get the following recursive definition for A(d, x):</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, 0) = A(d, A(d, 0))</span>
<span class="line">A(d + 1, x) = A(d, A(d + 1, x - 1))</span></code></pre>

</figure>
<p><span>As a Python program:</span></p>

<figure class="code-block">


<pre><code><span class="line">def A(d, x):</span>
<span class="line">  if d == 1: return x + 1</span>
<span class="line">  if x == 0: return A(d-1, A(d-1, 0))</span>
<span class="line">  return A(d-1, A(d, x - 1))</span></code></pre>

</figure>
<p><span>It</span>&rsquo;<span>s easy to see that computing </span><code>A</code><span> on a Turing Machine using this definition terminates </span>&mdash;<span> this</span>
<span>is a function with two arguments, and every recursive call uses lexicographically smaller pair of</span>
<span>arguments. And we constructed A in such a way that </span><code>A(d, x)</code><span> as a function of </span><code>x</code><span> is larger than any</span>
<span>PRF with a single argument of depth d. But that means that the following function with one argument</span>
<code class="display">a(x) = A(x, x) </code></p>
<p><span>grows faster than </span><em><span>any</span></em><span> PRF. And that</span>&rsquo;<span>s an example of a function which a Turing Machine have no</span>
<span>trouble computing (given sufficient time), but which is beyond the capabilities of PRF.</span></p>
</section>
<section id="Part-III-Descent-From-the-Ivory-Tower">

    <h2>
    <a href="#Part-III-Descent-From-the-Ivory-Tower"><span>Part III, Descent From the Ivory Tower</span> </a>
    </h2>
<p><span>Remember, this is a tree-part post! And are finally at the part 3! So let</span>&rsquo;<span>s circe back to the</span>
<span>practical matters. We have learned that:</span></p>
<ul>
<li>
<span>Turing machines don</span>&rsquo;<span>t necessary terminate.</span>
</li>
<li>
<span>While other computational devices, like FSMs and PRFs, can be made to always terminate, there</span>&rsquo;<span>s no</span>
<span>guarantee that they</span>&rsquo;<span>ll terminate fast. PRFs in particular, can compute quite large functions!</span>
</li>
<li>
<span>And non-Turing complete devices can be quiet expressive. For example, any real-world algorithm</span>
<span>that works on a TM can be adapted to run as a PRF.</span>
</li>
<li>
<span>Moreover, you don</span>&rsquo;<span>t even have to contort the algorithm much to make it fit. There</span>&rsquo;<span>s a universal</span>
<span>recipe for how to take something Turing complete, and make it a primitive recursive function</span>
<span>instead </span>&mdash;<span> just add an iteration counter to the device, and forcibly halt it if the counter grows</span>
<span>too large.</span>
</li>
</ul>
<p><span>Or, more succinctly: there</span>&rsquo;<span>s no practical difference between a program that doesn</span>&rsquo;<span>t terminate, and</span>
<span>the one that terminates after a billion years. As a practitioner, if you think you need to solve the</span>
<span>first problem, you need to solve the second problem as well. And making your programming language</span>
<span>non-Turing complete doesn</span>&rsquo;<span>t really help this.</span></p>
<p><span>And yet, there are a lot of configuration languages out there, that use non-Turing completeness as</span>
<span>one of the key design goal. Why is that?</span></p>
<p><span>I would say that we are never interested in Turing-completeness per-se. We usually want some </span><em><span>much</span></em>
<span>stronger properties. And yet, there</span>&rsquo;<span>s no convenient, catchy name for that bag of features of a good</span>
<span>configuration language. So, </span>&ldquo;<span>non-Turing-complete</span>&rdquo;<span> gets used as a sort of rallying cry to signal that</span>
<span>something is a good configuration language, and maybe sometimes even to justify to others inventing</span>
<span>a new language instead of taking something like Lua. That is, the </span><em><span>real</span></em><span> reason why you want at</span>
<span>least a different implementation is all those properties you really need, but they are kinda hard to</span>
<span>explain, or at least much harder than </span>&ldquo;<span>we can</span>&rsquo;<span>t use Python/Lua/JavaScript because they are</span>
<span>Turing-complete</span>&rdquo;<span>.</span></p>
<p><span>So what </span><em><span>are</span></em><span> the properties of a good configuration language?</span></p>
<p><em><span>First</span></em><span>, we need the language to be deterministic. If you launch Python and type </span><code>id([])</code><span>, you</span>&rsquo;<span>ll</span>
<span>see some number. If you hit </span><code>^C</code><span>, and than do this again, you</span>&rsquo;<span>ll see a different number. This is OK</span>
<span>for </span>&ldquo;<span>normal</span>&rdquo;<span> programming, but is usually anathema for configuration. Configuration is often use as a</span>
<span>key in some incremental, caching system, and letting in non-determinism there wrecks absolute chaos!</span></p>
<p><em><span>Second</span></em><span>, you need the language to be well-defined. You can compile Python with ASLR disabled, and</span>
<span>use some specific allocator, such that </span><code>id([])</code><span> always returns the same result. But that result</span>
<span>would be hard to predict! And if someone tries to do an alternative implementation, even if they</span>
<span>disable ASLR as well, they are likely to get a different deterministic number! Or the same could</span>
<span>happen if you just update the version of Python. So, the semantics of the language should be clearly</span>
<span>pinned-down by some sort of the reference, such that it is possible to guarantee not only</span>
<span>deterministic behavior, but fully identical behavior across different implementations.</span></p>
<p><em><span>Third</span></em><span>, you need the language to be pure. If your configuration can access environment variables or</span>
<span>read files on disk, than the meaning of the configuration would depend on the environment where the</span>
<span>configuration is evaluated, and you again don</span>&rsquo;<span>t want that, to make caching work.</span></p>
<p><em><span>Fourth</span></em><span>, a thing that is closely related to purity is security and sandboxing. The </span><em><span>mechanism</span></em><span> to</span>
<span>achieve both purity and security is the same </span>&mdash;<span> you don</span>&rsquo;<span>t expose general IO to your language. But</span>
<span>the purpose is different: purity is about not letting the results being non-deterministic, while</span>
<span>security is about not exposing access tokens to the attacker.</span></p>
<p><span>And now this gets tricky. One particular possible attack is a denial of service </span>&mdash;<span> sending some bad</span>
<span>config which makes our system to just spin there burning the CPU. Even if you control all IO, you</span>
<span>are generally still open to these kinds of attacks. It might be OK to say this is outside of the</span>
<span>threat model </span>&mdash;<span> that no one would find it valuable enough to just burn your CPU, if they can</span>&rsquo;<span>t also</span>
<span>do IO, and that, even in the event this happens, there</span>&rsquo;<span>s going to be some easy mitigation in the</span>
<span>form of higher-level timeout.</span></p>
<p><span>But you also might choose to provide some sort of guarantees about execution time, and that</span>&rsquo;<span>s really</span>
<span>hard. The two approaches work. One is to make sure that processing is </span><em><span>obviously linear</span></em><span>. Not just</span>
<span>terminates, but is actually proportional to the size of inputs, and in a very direct way. If the</span>
<span>correspondence is not direct, than it</span>&rsquo;<span>s highly likely that it is in fact non linear. The second</span>
<span>approach is to ensure </span><em><span>metered execution</span></em><span> </span>&mdash;<span> during processing, decrement a counter for every</span>
<span>simple atomic step and terminate processing when the counter reaches zero.</span></p>
<p><em><span>Finally</span></em><span> one more vague property you</span>&rsquo;<span>d want from a configuration language is for it to be simple.</span>
<span>That is, to ensure that, when people use your language, they write simple programs. It seems to me</span>
<span>that this might actually be the case where banning recursion and unbounded loops could help, though</span>
<span>I am not sure. As we know from the PRF exercise, this won</span>&rsquo;<span>t actually prevent people from writing</span>
<span>arbitrary recursive programs. It</span>&rsquo;<span>ll just require </span><a href="https://mochiro.moe/posts/09-meson-raytracer/"><span>some roundabout</span>
<span>code</span></a><span> to do that. But maybe that</span>&rsquo;<span>ll be enough of a</span>
<span>speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one?</span></p>
<p><span>That</span>&rsquo;<span>s all for today! Have a great weekend, and remember:</span></p>

<figure class="blockquote">
<blockquote><p><span>Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some</span>
<span>primitive recursive function of input can also be implemented by a primitive recursive function!</span></p>
</blockquote>

</figure>
</section>
]]></content>
</entry>

<entry>
<title type="text">How I Use Git Worktrees</title>
<link href="https://matklad.github.io/2024/07/25/git-worktrees.html" rel="alternate" type="text/html" title="How I Use Git Worktrees" />
<published>2024-07-25T00:00:00+00:00</published>
<updated>2024-07-25T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/07/25/git-worktrees</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[There are a bunch of posts on the internet about using git worktree command. As far as I can tell,
1most of them are primarily about using worktrees as a replacement of, or a supplement to git
branches. Instead of switching branches, you just change directories. This is also how I originally
had used worktrees, but that didn't stick, and I abandoned them. But recently worktrees grew
on me, though my new use-case is unlike branching.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/07/25/git-worktrees.html"><![CDATA[
<h1><span>How I Use Git Worktrees</span> <time class="meta" datetime="2024-07-25">Jul 25, 2024</time></h1>
<p><span>There are a bunch of posts on the internet about using </span><code>git worktree</code><span> command. As far as I can tell,</span>
<span>1most of them are primarily about using worktrees as a replacement of, or a supplement to git</span>
<span>branches. Instead of switching branches, you just change directories. This is also how I originally</span>
<span>had used worktrees, but that didn</span>&rsquo;<span>t stick, and I abandoned them. But recently worktrees grew</span>
<span>on me, though my new use-case is unlike branching.</span></p>
<section id="When-a-Branch-is-Enough">

    <h2>
    <a href="#When-a-Branch-is-Enough"><span>When a Branch is Enough</span> </a>
    </h2>
<p><span>If you use worktrees as a replacement for branching, that</span>&rsquo;<span>s great, no need to change anything! But</span>
<span>let me start with explaining why that workflow isn</span>&rsquo;<span>t for me.</span></p>
<p><span>The principal problem with using branches is that it</span>&rsquo;<span>s hard to context switch in the middle of doing</span>
<span>something. You have your branch, your commit, a bunch of changes in the work tree, some of them</span>
<span>might be stages and some unstaged. You can</span>&rsquo;<span>t really tell Git </span>&ldquo;<span>save all this context and restore it</span>
<span>later.</span>&rdquo;<span> The solution that Git suggests here is to use stashing, but that</span>&rsquo;<span>s awkward, as it is too</span>
<span>easy to get lost when stashing several things at the same time, and then applying the stash on top</span>
<span>of the wrong branch.</span></p>
<p><span>Managing Git state became much easier for me when I realized that the staging area and the stash are just bad</span>
<span>features, and life is easier if I avoid them. Instead, I just commit whatever and deal with</span>
<span>it later. So, when I need to switch a branch in the middle of things, what I do is, basically:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> git add .</span>
<span class="line"><span class="hl-title function_">$</span> git commit -m.</span>
<span class="line"><span class="hl-title function_">$</span> git switch another-branch</span></code></pre>

</figure>
<p><span>And, to switch back,</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> git switch -</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Undo the last commit, but keep its changes in the working tree</span></span>
<span class="line"><span class="hl-title function_">$</span> git reset HEAD~</span></code></pre>

</figure>
<p><span>To make this more streamlined, I have a </span><code>ggc</code><span> utility which does </span>&ldquo;<span>commit all with a trivial message</span>&rdquo;
<span>atomically.</span></p>

<aside class="admn note">
<svg class="icon"><use href="/assets/icons.svg#info"/></svg>
<div><p><span>Reminder: Git is not a version control system, Git is a toolbox for building a VCS. Do have a</span>
<span>low-friction way to add your own scripts for common git operations.</span></p>
</div>
</aside><p><span>And I don</span>&rsquo;<span>t always </span><code>reset HEAD~</code><span> </span>&mdash;<span> I usually just continue hacking with </span><code>.</code><span> in my Git log and then amend the commit</span>
<span>once I am satisfied with subset of changes</span></p>

<aside class="admn note">
<svg class="icon"><use href="/assets/icons.svg#info"/></svg>
<div><p><span>Reminder: magit, for </span><a href="https://magit.vc"><span>Emacs</span></a><span> and </span><a href="https://github.com/kahole/edamagit"><span>VS Code</span></a><span>, is</span>
<span>excellent for making such commit surgery easy. In particular, </span><strong><strong><span>instant fixup</span></strong></strong><span> is excellent. Even</span>
<span>if you don</span>&rsquo;<span>t use magit, you should have an equivalent of instant fixup among your Git scripts.</span></p>
</div>
</aside><p><span>So that</span>&rsquo;<span>s how I deal with switching branches. But why worktrees then?</span></p>
</section>
<section id="Worktree-Per-Concurrent-Activity">

    <h2>
    <a href="#Worktree-Per-Concurrent-Activity"><span>Worktree Per Concurrent Activity</span> </a>
    </h2>
<p><span>It</span>&rsquo;<span>s a bit hard to describe, but:</span></p>
<ul>
<li>
<span>I have a fixed number of worktrees (5, to be exact)</span>
</li>
<li>
<span>worktrees are mostly uncorrelated to branches</span>
</li>
<li>
<span>but instead correspond to my concurrent activities during coding.</span>
</li>
</ul>
<p><span>Specifically:</span></p>
<ul>
<li>
<p><span>The </span><strong><span>main</span></strong><span> worktree is a readonly worktree that contains a recent snapshot of the remote main</span>
<span>branch. I use this tree to compare the code I am currently working on and/or reviewing with the</span>
<span>master version (this includes things like </span>&ldquo;<span>how long the build takes</span>&rdquo;<span>, </span>&ldquo;<span>what is the behavior of</span>
<span>this test</span>&rdquo;<span> and the like, so not just the actual source code).</span></p>
</li>
<li>
<p><span>The </span><strong><span>work</span></strong><span> worktree, where I write most of the code. I often need to write new code and compare it</span>
<span>with old code at the same time. But can</span>&rsquo;<span>t actually work on two different things in parallel.</span>
<span>That</span>&rsquo;<span>s why </span><code>main</code><span> and </span><code>work</code><span> are different worktrees, but </span><code>work</code><span> also constantly switches branches.</span></p>
</li>
<li>
<p><span>The </span><strong><span>review</span></strong><span> worktree, where I checkout code for code review. While I can</span>&rsquo;<span>t review code and write</span>
<span>code at the same time, there is one thing I am implementing, and one thing I am reviewing, but the</span>
<span>review and implementation proceed concurrently.</span></p>
</li>
<li>
<p><span>Then, there</span>&rsquo;<span>s the </span><strong><span>fuzz</span></strong><span> tree, where I run long-running fuzzing jobs for the code I am actively working</span>
<span>on. My overall idealized feature workflow looks like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># go to the `work` worktree</span></span>
<span class="line"><span class="hl-title function_">$</span> cd ~/projects/tigerbeetle/work</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Create a new branch. As we work with a centralized repo,</span></span>
<span class="line"><span class="hl-comment"># rather than personal forks, I tend to prefix my branch names</span></span>
<span class="line"><span class="hl-comment"># with `matklad/`</span></span>
<span class="line"><span class="hl-title function_">$</span> git switch -c matklad/awesome-feature</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Start with a reasonably clean slate.</span></span>
<span class="line"><span class="hl-comment"># In reality, I have yet another script to start a branch off</span></span>
<span class="line"><span class="hl-comment"># fresh from the main remote, but this reset is a good enough approximation.</span></span>
<span class="line"><span class="hl-title function_">$</span> git reset --hard origin/main</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># For more complicated features, I start with an empty commit</span></span>
<span class="line"><span class="hl-comment"># and write the commit message _first_, before starting the work.</span></span>
<span class="line"><span class="hl-comment"># That's a good way to collect your thoughts and discover dead</span></span>
<span class="line"><span class="hl-comment"># ends more gracefully than hitting a brick wall coding at 80 WPM.</span></span>
<span class="line"><span class="hl-title function_">$</span> git commit --allow-empty</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Hack furiously writing throughway code.</span></span>
<span class="line"><span class="hl-title function_">$</span> code .</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># At this point, I have something that I hope works</span></span>
<span class="line"><span class="hl-comment"># but would be embarrassed to share with anyone!</span></span>
<span class="line"><span class="hl-comment"># So that's the good place to kick off fuzzing.</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># First, I commit everything so far.</span></span>
<span class="line"><span class="hl-comment"># Remember, I have `ggc` one liner for this:</span></span>
<span class="line"><span class="hl-title function_">$</span> git add . &amp;&amp; git commit -m.</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Now I go to my `fuzz` worktree and kick off fuzzing.</span></span>
<span class="line"><span class="hl-comment"># I usually split screen here.</span></span>
<span class="line"><span class="hl-comment"># On the left, I copy the current commit hash.</span></span>
<span class="line"><span class="hl-comment"># On the right, I switch to the fuzzing worktree,</span></span>
<span class="line"><span class="hl-comment"># switch to the copied commit, and start fuzzing:</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span> git add . &amp;&amp; git commit -m.  |</span>
<span class="line"><span class="hl-title function_">$</span> git rev-parse HEAD | ctrlc   | $ cd ../fuzz</span>
<span class="line"><span class="hl-title function_">$</span>                              | $ git switch -d $(ctrlv)</span>
<span class="line"><span class="hl-title function_">$</span>                              | $ ./zig/zig build fuzz</span>
<span class="line"><span class="hl-title function_">$</span>                              |</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># While the fuzzer hums on the right, I continue to furiously refactor</span></span>
<span class="line"><span class="hl-comment"># the code on the left and hammer my empty commit with a wishful</span></span>
<span class="line"><span class="hl-comment"># thinking message and my messy code commit with `.` message into</span></span>
<span class="line"><span class="hl-comment"># a semblance of clean git history</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span> code .</span>
<span class="line"><span class="hl-title function_">$</span> magit-goes-brrrrr</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># At this point, in the work tree, I am happy with both the code</span></span>
<span class="line"><span class="hl-comment"># and the Git history, so, if the fuzzer on the right is happy,</span></span>
<span class="line"><span class="hl-comment"># a PR is opened!</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span>                              |</span>
<span class="line"><span class="hl-title function_">$</span> git push --force-with-lease  | $ ./zig/zig build fuzz</span>
<span class="line"><span class="hl-title function_">$</span> gh pr create --web           | # Still hasn't failed</span>
<span class="line"><span class="hl-title function_">$</span>                              |</span></code></pre>

</figure>
<p><span>This is again concurrent: I can hack on the branch while the fuzzer tests the </span>&ldquo;<span>same</span>&rdquo;<span> code. Note</span>
<span>that it is crucial that the fuzzing tree operates in the detached head state (</span><code>-d</code><span> flag for </span><code>git</code>
<code>switch</code><span>). In general, </span><code>-d</code><span> is very helpful with this style of worktree work. I am also</span>
<span>sympathetic to </span><a href="https://martinvonz.github.io/jj/latest/"><span>the argument</span></a><span> that, like the staging area</span>
<span>and the stash, Git branches are a misfeature, but I haven</span>&rsquo;<span>t made the plunge personally yet.</span></p>
</li>
<li>
<p><span>Finally, the last tree I have is </span><strong><span>scratch</span></strong><span> </span>&ndash;<span> this is a tree for arbitrary random things I need</span>
<span>to do while working on something else. For example, if I am working on </span><code>matklad/my-feature</code><span> in</span>
<code>work</code><span>, and reviewing </span><code>#6292</code><span> in </span><code>review</code><span>, and, while reviewing, notice a tiny unrelated typo, the</span>
<span>PR for that typo is quickly prepped in the </span><code>scratch</code><span> worktree:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> cd ../scratch</span>
<span class="line"><span class="hl-title function_">$</span> git switch -c matklad/quick-fix</span>
<span class="line"><span class="hl-title function_">$</span> code . &amp;&amp; git add . &amp;&amp; git commit -m 'typo' &amp;&amp; git push</span>
<span class="line"><span class="hl-title function_">$</span> cd -</span></code></pre>

</figure>
</li>
</ul>
<p><span>TL;DR: consider using worktrees not as a replacement for branches, but as a means to manage</span>
<span>concurrency in your tasks. My level of concurrency is:</span></p>
<ul>
<li>
<code>main</code><span> for looking at the pristine code,</span>
</li>
<li>
<code>work</code><span> for looking at my code,</span>
</li>
<li>
<code>review</code><span> for looking at someone else</span>&rsquo;<span>s code,</span>
</li>
<li>
<code>fuzz</code><span> for my computer to look at my code,</span>
</li>
<li>
<code>scratch</code><span> for everything else!</span>
</li>
</ul>
</section>
]]></content>
</entry>

<entry>
<title type="text">Properly Testing Concurrent Data Structures</title>
<link href="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html" rel="alternate" type="text/html" title="Properly Testing Concurrent Data Structures" />
<published>2024-07-05T00:00:00+00:00</published>
<updated>2024-07-05T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[There's a fascinating Rust library, loom, which can be used to
thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But
recently I accidentally implemented a small toy which, I think, contains some of the loom's ideas,
and it seems worthwhile to write about that. The goal here isn't to teach you what you should be
using in practice (if you need that, go read loom's docs), but rather to derive a couple of neat
ideas from first principles.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html"><![CDATA[
<h1><span>Properly Testing Concurrent Data Structures</span> <time class="meta" datetime="2024-07-05">Jul 5, 2024</time></h1>
<p><span>There</span>&rsquo;<span>s a fascinating Rust library, </span><a href="https://github.com/tokio-rs/loom"><span>loom</span></a><span>, which can be used to</span>
<span>thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But</span>
<span>recently I accidentally implemented a small toy which, I think, contains some of the loom</span>&rsquo;<span>s ideas,</span>
<span>and it seems worthwhile to write about that. The goal here isn</span>&rsquo;<span>t to teach you what you should be</span>
<span>using in practice (if you need that, go read loom</span>&rsquo;<span>s docs), but rather to derive a couple of neat</span>
<span>ideas from first principles.</span></p>
<section id="One-Two-Three-Two">

    <h2>
    <a href="#One-Two-Three-Two"><span>One, Two, Three, Two</span> </a>
    </h2>
<p><span>As usual, we need the simplest possible model program to mess with. The example we use comes from</span>
<a href="https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html"><span>this excellent article</span></a><span>.</span>
<span>Behold, a humble (and broken) concurrent counter:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::{</span>
<span class="line">  AtomicU32,</span>
<span class="line">  Ordering::SeqCst,</span>
<span class="line">};</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The bug is obvious here </span>&mdash;<span> the increment is not atomic. But what is the best test we can write to</span>
<span>expose it?</span></p>
</section>
<section id="Trivial-Test">

    <h2>
    <a href="#Trivial-Test"><span>Trivial Test</span> </a>
    </h2>
<p><span>The simplest idea that comes to mind is to just hammer the same counter from multiple threads and</span>
<span>check the result at the end;</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">threaded_test</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">thread_count</span> = <span class="hl-number">100</span>;</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">increment_count</span> = <span class="hl-number">100</span>;</span>
<span class="line"></span>
<span class="line">  std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..thread_count {</span>
<span class="line">      scope.<span class="hl-title function_ invoke__">spawn</span>(|| {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..increment_count {</span>
<span class="line">          counter.<span class="hl-title function_ invoke__">increment</span>()</span>
<span class="line">        }</span>
<span class="line">      });</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line"></span>
<span class="line">  <span class="hl-built_in">assert_eq!</span>(counter.<span class="hl-title function_ invoke__">get</span>(), thread_count * increment_count);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>This fails successfully:</span></p>

<figure class="code-block">


<pre><code><span class="line">thread 'counter::trivial' panicked:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 9598</span>
<span class="line"> right: 10000</span></code></pre>

</figure>
<p><span>But I wouldn</span>&rsquo;<span>t call this test satisfactory </span>&mdash;<span> it very much depends on the timing, so you can</span>&rsquo;<span>t</span>
<span>reproduce it deterministically and you can</span>&rsquo;<span>t debug it. You also can</span>&rsquo;<span>t minimize it </span>&mdash;<span> if you reduce</span>
<span>the number of threads and increments, chances are the test passes by luck!</span></p>
</section>
<section id="PBT">

    <h2>
    <a href="#PBT"><span>PBT</span> </a>
    </h2>
<p><span>Of course the temptation is to apply property based testing here! The problem </span><em><span>almost</span></em><span> fits: we have</span>
<span>easy-to-generate input (the sequence of increments spread over several threads), a good property to</span>
<span>check (result of concurrent increments is identical to that of sequential execution) and the desire</span>
<span>to minimize the test.</span></p>
<p><span>But just how can we plug threads into a property-based test?</span></p>
<p><span>PBTs are great for testing state machines. You can run your state machine through a series of steps</span>
<span>where at each step a PBT selects an arbitrary next action to apply to the state:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">state_machine_test</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    <span class="hl-comment">// This is our state machine!</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">state</span>: <span class="hl-type">i32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// We&#x27;ll run it for up to 100 steps.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">step_count</span>: <span class="hl-type">usize</span> = rng.<span class="hl-title function_ invoke__">int_in_range</span>(<span class="hl-number">0</span>..=<span class="hl-number">100</span>)?;</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..step_count {</span>
<span class="line">      <span class="hl-comment">// At each step, we flip a coin and</span></span>
<span class="line">      <span class="hl-comment">// either increment or decrement.</span></span>
<span class="line">      <span class="hl-keyword">match</span> *rng.<span class="hl-title function_ invoke__">choose</span>(&amp;[<span class="hl-string">&quot;inc&quot;</span>, <span class="hl-string">&quot;dec&quot;</span>])? {</span>
<span class="line">        <span class="hl-string">&quot;inc&quot;</span> =&gt; state += <span class="hl-number">1</span>,</span>
<span class="line">        <span class="hl-string">&quot;dec&quot;</span> =&gt; state -= <span class="hl-number">1</span>,</span>
<span class="line">        _ =&gt; <span class="hl-built_in">unreachable!</span>(),</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And it </span><em><span>feels</span></em><span> like we should be able to apply the same technique here. At every iteration, pick a</span>
<span>random thread and make it do a single step. If you can step the threads manually, it should be easy</span>
<span>to maneuver one thread in between load&amp;store of a different thread.</span></p>
<p><span>But we can</span>&rsquo;<span>t step through threads! Or can we?</span></p>
</section>
<section id="Simple-Instrumentation">

    <h2>
    <a href="#Simple-Instrumentation"><span>Simple Instrumentation</span> </a>
    </h2>
<p><span>Ok, let</span>&rsquo;<span>s fake it until we make it! Let</span>&rsquo;<span>s take a look at the buggy increment method:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">  <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ideally, we</span>&rsquo;<span>d love to be able to somehow </span>&ldquo;<span>pause</span>&rdquo;<span> the thread in-between atomic operations. Something</span>
<span>like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">    <span class="hl-comment">// ¯\_(ツ)_/¯</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s start with implementing our own wrapper for </span><code>AtomicU32</code><span> which includes calls to pause.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::Ordering;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  inner: std::sync::atomic::AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">load</span>(&amp;<span class="hl-keyword">self</span>, ordering: Ordering) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">load</span>(ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">store</span>(&amp;<span class="hl-keyword">self</span>, value: <span class="hl-type">u32</span>, ordering: Ordering) {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">store</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-comment">// still no idea :(</span></span>
<span class="line">}</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-API">

    <h2>
    <a href="#Managed-Threads-API"><span>Managed Threads API</span> </a>
    </h2>
<p><span>One rule of a great API design is that you start by implement a single </span><em><span>user</span></em><span> of an API, to</span>
<span>understand how the API should </span><em><span>feel</span></em><span>, and only then proceed to the actual implementation.</span></p>
<p><span>So, in the spirit of faking, let</span>&rsquo;<span>s just write a PBT using these pausable, managed threads, even if</span>
<span>we still have no idea how to actually implement pausing.</span></p>
<p><span>We start with creating a counter and two managed threads. And we probably want to pass a reference</span>
<span>to the counter to each of the threads:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span></code></pre>

</figure>
<p><span>Now, we want to step through the threads:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">coin_flip</span>: <span class="hl-type">bool</span> = rng.<span class="hl-title function_ invoke__">arbitrary</span>()?;</span>
<span class="line">  <span class="hl-keyword">if</span> t1.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">    <span class="hl-keyword">if</span> coin_flip {</span>
<span class="line">      t1.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">  <span class="hl-keyword">if</span> t2.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">    <span class="hl-keyword">if</span> coin_flip {</span>
<span class="line">      t2.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Or, refactoring this a bit to semantically compress:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">  <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> threads {</span>
<span class="line">    <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() &amp;&amp; rng.<span class="hl-title function_ invoke__">arbitrary</span>()? {</span>
<span class="line">      t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>That is, on each step of our state machine, we loop through all threads and unpause a random subset</span>
<span>of them.</span></p>
<p><span>But besides pausing and unpausing, we need our threads to actually </span><em><span>do</span></em><span> something, to increment the</span>
<span>counter. One idea is to mirror the </span><code>std::spawn</code><span> API and pass a closure in:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = &amp;counter;</span>
<span class="line">  <span class="hl-keyword">move</span> || {</span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..<span class="hl-number">100</span> {</span>
<span class="line">      counter.<span class="hl-title function_ invoke__">increment</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">});</span></code></pre>

</figure>
<p><span>But as these are managed threads, and we want to control them from our tests, lets actually go all</span>
<span>the way there and give the controlling thread an ability to change the code running in a managed</span>
<span>thread. That is, we</span>&rsquo;<span>ll start managed threads without a </span>&ldquo;<span>main</span>&rdquo;<span> function, and provide an API to</span>
<span>execute arbitrary closures in the context of this by-default inert thread (</span><a href="https://joearms.github.io/published/2013-11-21-My-favorite-erlang-program.html"><span>universal</span>
<span>server</span></a><span> anyone?):</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line"><span class="hl-comment">// We pass the state, &amp;counter, in, but otherwise the thread is inert.</span></span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"></span>
<span class="line"><span class="hl-comment">// But we can manually poke it:</span></span>
<span class="line">t.<span class="hl-title function_ invoke__">submit</span>(|thread_state: &amp;Counter| thread_state.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">t.<span class="hl-title function_ invoke__">submit</span>(|thread_state: &amp;Counter| thread_state.<span class="hl-title function_ invoke__">increment</span>());</span></code></pre>

</figure>
<p><span>Putting everything together, we get a nice-looking property test:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[cfg(test)]</span></span>
<span class="line"><span class="hl-keyword">use</span> managed_thread::AtomicU32;</span>
<span class="line"><span class="hl-meta">#[cfg(not(test))]</span></span>
<span class="line"><span class="hl-keyword">use</span> std::sync::atomic::AtomicU32;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-comment">// ...</span></span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    <span class="hl-comment">// Our &quot;Concurrent System Under Test&quot;.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// The sequential model we&#x27;ll compare the result against.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// Two managed threads which we will be stepping through</span></span>
<span class="line">    <span class="hl-comment">// manually.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// Bulk of the test: in a loop, flip a coin and advance</span></span>
<span class="line">    <span class="hl-comment">// one of the threads.</span></span>
<span class="line">    <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> [t1, t2] {</span>
<span class="line">        <span class="hl-keyword">if</span> rng.<span class="hl-title function_ invoke__">arbitrary</span>() {</span>
<span class="line">          <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">            t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">          } <span class="hl-keyword">else</span> {</span>
<span class="line">            <span class="hl-comment">// Standard &quot;model equivalence&quot; property: apply</span></span>
<span class="line">            <span class="hl-comment">// isomorphic actions to the system and its model.</span></span>
<span class="line">            t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">            counter_model += <span class="hl-number">1</span>;</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> threads {</span>
<span class="line">      t.<span class="hl-title function_ invoke__">join</span>();</span>
<span class="line">    }</span>
<span class="line"></span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line"></span>
<span class="line">    <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Now, if only we could make this API work</span>&hellip;<span> Remember, our </span><code>pause</code><span> implementation is a shrug emoji!</span></p>
<p><span>At this point, you might be mightily annoyed at me for this rhetorical device where I pretend that I</span>
<span>don</span>&rsquo;<span>t know the answer. No need for annoyance </span>&mdash;<span> when writing this code for the first time, I traced</span>
<span>exactly these steps </span>&mdash;<span> I realized that I need a </span>&ldquo;<span>pausing </span><code>AtomicU32</code>&rdquo;<span> so I did that (with dummy</span>
<span>pause calls), then I played with the API I </span><em><span>wanted</span></em><span> to have, ending at roughly this spot, without</span>
<span>yet knowing how I would make it work or, indeed, if it is possible at all.</span></p>
<p><span>Well, if I am being honest, there is a bit of up-front knowledge here. I don</span>&rsquo;<span>t think we can avoid</span>
<span>spawning real threads here, unless we do something really cursed with inline assembly. When</span>
<em><span>something</span></em><span> calls that </span><code>pause()</code><span> function, and we want it to stay paused until further notice, that</span>
<span>just has to happen in a thread which maintains a stack separate from the stack of our test. And, if</span>
<span>we are going to spawn threads, we might as well spawn scoped threads, so that we can freely borrow</span>
<span>stack-local data. And to spawn a scope thread, you need a</span>
<a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html"><code>Scope</code></a><span> parameter. So in reality</span>
<span>we</span>&rsquo;<span>ll need one more level of indentation here:</span></p>

<figure class="code-block">


<pre><code><span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line">      <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> [t1, t2] {</span>
<span class="line">          <span class="hl-comment">// ...</span></span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line">    });</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-Implementation">

    <h2>
    <a href="#Managed-Threads-Implementation"><span>Managed Threads Implementation</span> </a>
    </h2>
<p><span>Now, the fun part: how the heck are we going to make pausing and unpausing work? For starters, there</span>
<span>clearly needs to be some communication between the main thread (</span><code>t.unpause()</code><span>) and the managed</span>
<span>thread (</span><code>pause()</code><span>). And, because we don</span>&rsquo;<span>t want to change </span><code>Counter</code><span> API to thread some kind of</span>
<span>test-only context, the context needs to be smuggled. So </span><code>thread_local!</code><span> it is. And this context</span>
<span>is going to be shared between two threads, so it must be wrapped in an </span><code>Arc</code><span>.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-comment">// 🤷</span></span>
<span class="line">}</span>
<span class="line"></span>
<span class="line">thread_local! {</span>
<span class="line">  <span class="hl-keyword">static</span> INSTANCE: RefCell&lt;<span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span class="line">    RefCell::<span class="hl-title function_ invoke__">new</span>(<span class="hl-literal">None</span>);</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| *it.<span class="hl-title function_ invoke__">borrow_mut</span>() = <span class="hl-title function_ invoke__">Some</span>(ctx));</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>() <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| it.<span class="hl-title function_ invoke__">borrow</span>().<span class="hl-title function_ invoke__">clone</span>())</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>As usual when using </span><code>thread_local!</code><span> or </span><code>lazy_static!</code><span>, it is convenient to immediately wrap it into</span>
<span>better typed accessor functions. And, given that we are using an </span><code>Arc</code><span> here anyway, we can</span>
<span>conveniently escape </span><code>thread_local</code>&rsquo;<span>s </span><code>with</code><span> by cloning the </span><code>Arc</code><span>.</span></p>
<p><span>So now we finally can implement the global </span><code>pause</code><span> function (or at least can kick the proverbial can</span>
<span>a little bit farther):</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-keyword">if</span> <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(ctx) = SharedContext::<span class="hl-title function_ invoke__">get</span>() {</span>
<span class="line">    ctx.<span class="hl-title function_ invoke__">pause</span>()</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-comment">// 😕</span></span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ok, what to do next? We somehow need to coordinate the control thread and the managed thread. And we</span>
<span>need some sort of notification mechanism, so that the managed thread knows when it can continue. The</span>
<span>most brute force solution here is a pair of a mutex protecting some state and a condition variable.</span>
<span>Mutex guards the state that can be manipulated by either of the threads. Condition variable can be</span>
<span>used to signal about the changes.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  state: Mutex&lt;State&gt;,</span>
<span class="line">  cv: Condvar,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-comment">// 🤡</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Okay, it looks like I am running out of emojies here. There</span>&rsquo;<span>s no more layers of indirection or</span>
<span>infrastructure left, we need to write some real code that actually does do that pausing thing. So</span>
<span>let</span>&rsquo;<span>s say that the state is tracking, well, the state of our managed thread, which can be either</span>
<span>running or paused:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[derive(PartialEq, Eq, Default)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-meta">#[default]</span></span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And then the logic of the pause function </span>&mdash;<span> flip the state from </span><code>Running</code><span> to </span><code>Paused</code><span>, notify the</span>
<span>controlling thread that we are </span><code>Paused</code><span>, and wait until the controlling thread flips our state back</span>
<span>to </span><code>Running</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    <span class="hl-keyword">while</span> *guard == State::Paused {</span>
<span class="line">      guard = <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">wait</span>(guard).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Aside: Rust</span>&rsquo;<span>s API for condition variables is beautiful. Condvars are tricky, and I didn</span>&rsquo;<span>t really</span>
<span>understood them until seeing the signatures of Rust functions. Notice how the </span><code>wait</code><span> function</span>
<em><span>takes</span></em><span> a mutex guard as an argument, and returns a mutex guard. This protects you from the logical</span>
<span>races and guides you towards the standard pattern of using condvars:</span></p>
<p><span>First, you lock the mutex around the shared state. Then, you inspect whether the state is what you</span>
<span>need. If that</span>&rsquo;<span>s the case, great, you do what you wanted to do and unlock the mutex. If not, then,</span>
<em><span>while still holding the mutex</span></em><span>, you </span><em><span>wait</span></em><span> on the condition variable. Which means that the</span>
<span>mutex gets unlocked, and other threads get the chance to change the shared state. When they do</span>
<span>change it, and notify the condvar, your thread wakes up, and it gets the locked mutex back (but the</span>
<span>state now is different). Due to the possibility of spurious wake-ups, you need to double check the</span>
<span>state and be ready to loop back again to waiting.</span></p>
<p><span>Naturally, there</span>&rsquo;<span>s a helper that encapsulates this whole pattern:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Paused)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ok, this actually does look like a reasonable implementation of </span><code>pause</code><span>. Let</span>&rsquo;<span>s move on to</span>
<code>managed_thread::spawn</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  state: T,</span>
<span class="line">) {</span>
<span class="line">  <span class="hl-comment">// ? ? ?? ??? ?????</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>There</span>&rsquo;<span>s a bunch of stuff that needs to happen here:</span></p>
<ul>
<li>
<span>As we have established, we are going to spawn a (scoped) thread, so we need the </span><code>scope</code><span> parameter</span>
<span>with its three lifetimes. I don</span>&rsquo;<span>t know how it works, so I am just going by the docs here!</span>
</li>
<li>
<span>We are going to return some kind of handle, which we can use to pause and unpause our managed</span>
<span>thread. And that handle is going to be parametrized over the same </span><code>'scope</code><span> lifetime, because it</span>&rsquo;<span>ll</span>
<span>hold onto the actual join handle.</span>
</li>
<li>
<span>We are going to pass the generic state to our new thread, and that state needs to be </span><code>Send</code><span>, and</span>
<span>bounded by the same lifetime as our scoped thread.</span>
</li>
<li>
<span>Inside, we are going to spawn a thread for sure, and we</span>&rsquo;<span>ll need to setup the </span><code>INSTANCE</code><span> thread</span>
<span>local on that thread.</span>
</li>
<li>
<span>And it would actually be a good idea to stuff a reference to that </span><code>SharedContext</code><span> into the handle</span>
<span>we return.</span>
</li>
</ul>
<p><span>A bunch of stuff, in other words. Let</span>&rsquo;<span>s do it:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(ctx);</span>
<span class="line">      <span class="hl-title function_ invoke__">drop</span>(state); <span class="hl-comment">// <span class="hl-doctag">TODO:</span> ¿</span></span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The essentially no-op function we spawn looks sus. We</span>&rsquo;<span>ll fix later! Let</span>&rsquo;<span>s try to implement</span>
<code>is_paused</code><span> and </span><code>unpause</code><span> first! They should be relatively straightforward. For </span><code>is_paused</code><span>, we just</span>
<span>need to lock the mutex and check the state:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">is_paused</span>(&amp;<span class="hl-keyword">self</span>,) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    *guard == State::Paused</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>For </span><code>unpause</code><span>, we should additionally flip the state back to </span><code>Running</code><span> and notify the other thread:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>But I think that</span>&rsquo;<span>s not quite correct. Can you see why?</span></p>
<p><span>With this implementation, after </span><code>unpause</code><span>, the controlling and the managed threads will be running</span>
<span>concurrently. And that can lead to non-determinism, the very problem we are trying to avoid here! In</span>
<span>particular, if you call </span><code>is_paused</code><span> </span><em><span>right</span></em><span> after you </span><code>unpause</code><span> the thread, you</span>&rsquo;<span>ll most likely get</span>
<code>false</code><span> back, as the other thread will still be running. But it might also hit the </span><em><span>next</span></em><span> </span><code>pause</code>
<span>call, so, depending on timing, you might also get </span><code>true</code><span>.</span></p>
<p><span>What we want is actually completely eliminating all unmanaged concurrency. That means that at any</span>
<span>given point in time, only one thread (controlling or managed) should be running. So the right</span>
<span>semantics for </span><code>unpause</code><span> is to unblock the managed thread, and then block the controlling thread</span>
<span>until the managed one hits the next pause!</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>At this point we can spawn a managed thread, pause it and resume. But right now it doesn</span>&rsquo;<span>t do</span>
<span>anything. Next step is implementing that idea where the controlling thread can directly send an</span>
<span>arbitrary closure to the managed one to make it do something:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: FnSomething&gt;(&amp;<span class="hl-keyword">self</span>, f: F)</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Let</span>&rsquo;<span>s figure this </span><code>FnSomething</code><span> bound! We are going to yeet this </span><code>f</code><span> over to the managed thread and</span>
<span>run it there once, so it is </span><code>FnOnce</code><span>. It is crossing thread-boundary, so it needs to be </span><code>+ Send</code><span>.</span>
<span>And, because we are using scoped threads, it </span><em><span>doesn</span>&rsquo;<span>t</span></em><span> have to be </span><code>'static</code><span>, just </span><code>'scope</code><span> is</span>
<span>enough. Moreover, in that managed thread the </span><code>f</code><span> will have exclusive access to thread</span>&rsquo;<span>s state, </span><code>T</code><span>.</span>
<span>So we have:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(<span class="hl-keyword">self</span>, f: F)</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Implementing this is a bit tricky. First, we</span>&rsquo;<span>ll need some sort of the channel to actually move the</span>
<span>function. Then, similarly to the </span><code>unpause</code><span> logic, we</span>&rsquo;<span>ll need synchronization to make sure that the</span>
<span>control thread doesn</span>&rsquo;<span>t resume until the managed thread starts running </span><code>f</code><span> and hits a pause (or maybe</span>
<span>completes </span><code>f</code><span>). And we</span>&rsquo;<span>ll also need a new state, </span><code>Ready</code><span>, because now there are two different</span>
<span>reasons why a managed thread might be blocked </span>&mdash;<span> it might wait for an </span><code>unpause</code><span> event, or it might</span>
<span>wait for the next </span><code>f</code><span> to execute. This is the new code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line hl-line">  <span class="hl-meta">#[default]</span></span>
<span class="line hl-line">  Ready,</span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line hl-line"><span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line hl-line">  sender: mpsc::Sender&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  <span class="hl-keyword">mut</span> state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line hl-line">  <span class="hl-keyword">let</span> (sender, receiver) =</span>
<span class="line hl-line">    mpsc::channel::&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx));</span>
<span class="line"></span>
<span class="line hl-line">      <span class="hl-keyword">for</span> <span class="hl-variable">f</span> <span class="hl-keyword">in</span> receiver {</span>
<span class="line hl-line">        <span class="hl-title function_ invoke__">f</span>(&amp;<span class="hl-keyword">mut</span> state);</span>
<span class="line hl-line"></span>
<span class="line hl-line">        <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">        <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line hl-line">        *guard = State::Ready;</span>
<span class="line hl-line">        ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>()</span>
<span class="line hl-line">      }</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx, sender }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(&amp;<span class="hl-keyword">self</span>, f: F) {</span>
<span class="line hl-line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Ready);</span>
<span class="line hl-line">    *guard = State::Running;</span>
<span class="line hl-line">    <span class="hl-keyword">self</span>.sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(f)).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">    guard = <span class="hl-keyword">self</span></span>
<span class="line hl-line">      .ctx</span>
<span class="line hl-line">      .cv</span>
<span class="line hl-line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line hl-line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The last small piece of the puzzle is the </span><code>join</code><span> function. It</span>&rsquo;<span>s </span><em><span>almost</span></em><span> standard! First we close</span>
<span>our side of the channel. This serves as a natural stop signal for the other thread, so it exits.</span>
<span>Which in turn allows us to join it. The small wrinkle here is that the thread might be paused when</span>
<span>we try to join it, so we need to unpause it beforehand:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">      <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">drop</span>(<span class="hl-keyword">self</span>.sender);</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">join</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s it! Let</span>&rsquo;<span>s put everything together!</span></p>
<p><span>Helper library, </span><code>managed_thread.rs</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::{</span>
<span class="line">  cell::RefCell,</span>
<span class="line">  sync::{atomic::Ordering, mpsc, Arc, Condvar, Mutex},</span>
<span class="line">  thread::Scope,</span>
<span class="line">};</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  inner: std::sync::atomic::AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">load</span>(&amp;<span class="hl-keyword">self</span>, ordering: Ordering) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">load</span>(ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">store</span>(&amp;<span class="hl-keyword">self</span>, value: <span class="hl-type">u32</span>, ordering: Ordering) {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">store</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-keyword">if</span> <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(ctx) = SharedContext::<span class="hl-title function_ invoke__">get</span>() {</span>
<span class="line">    ctx.<span class="hl-title function_ invoke__">pause</span>()</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  state: Mutex&lt;State&gt;,</span>
<span class="line">  cv: Condvar,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default, PartialEq, Eq, Debug)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-meta">#[default]</span></span>
<span class="line">  Ready,</span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line">thread_local! {</span>
<span class="line">  <span class="hl-keyword">static</span> INSTANCE: RefCell&lt;<span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span class="line">    RefCell::<span class="hl-title function_ invoke__">new</span>(<span class="hl-literal">None</span>);</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| *it.<span class="hl-title function_ invoke__">borrow_mut</span>() = <span class="hl-title function_ invoke__">Some</span>(ctx));</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>() <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| it.<span class="hl-title function_ invoke__">borrow</span>().<span class="hl-title function_ invoke__">clone</span>())</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Paused)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running)</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  sender: mpsc::Sender&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  <span class="hl-keyword">mut</span> state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> (sender, receiver) =</span>
<span class="line">    mpsc::channel::&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx));</span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">f</span> <span class="hl-keyword">in</span> receiver {</span>
<span class="line">        <span class="hl-title function_ invoke__">f</span>(&amp;<span class="hl-keyword">mut</span> state);</span>
<span class="line">        <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">        <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">        *guard = State::Ready;</span>
<span class="line">        ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>()</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx, sender }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">is_paused</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    *guard == State::Paused</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(&amp;<span class="hl-keyword">self</span>, f: F) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Ready);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(f)).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">      <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">drop</span>(<span class="hl-keyword">self</span>.sender);</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">join</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>System under test, not-exactly-atomic counter:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::Ordering::SeqCst;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[cfg(test)]</span></span>
<span class="line"><span class="hl-keyword">use</span> managed_thread::AtomicU32;</span>
<span class="line"><span class="hl-meta">#[cfg(not(test))]</span></span>
<span class="line"><span class="hl-keyword">use</span> std::sync::atomic::AtomicU32;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And the test itself:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line">      <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">        <span class="hl-title function_ invoke__">for</span> (tid, t) <span class="hl-keyword">in</span> threads.<span class="hl-title function_ invoke__">iter_mut</span>().<span class="hl-title function_ invoke__">enumerate</span>() {</span>
<span class="line">          <span class="hl-keyword">if</span> rng.<span class="hl-title function_ invoke__">arbitrary</span>()? {</span>
<span class="line">            <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">              eprintln!(<span class="hl-string">&quot;{tid}: unpause&quot;</span>);</span>
<span class="line">              t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">            } <span class="hl-keyword">else</span> {</span>
<span class="line">              eprintln!(<span class="hl-string">&quot;{tid}: increment&quot;</span>);</span>
<span class="line">              t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">              counter_model += <span class="hl-number">1</span>;</span>
<span class="line">            }</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line"></span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> threads {</span>
<span class="line">        t.<span class="hl-title function_ invoke__">join</span>();</span>
<span class="line">      }</span>
<span class="line">      <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line"></span>
<span class="line">      <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">    })</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Running it identifies a failure:</span></p>

<figure class="code-block">


<pre><code><span class="line">---- test_counter stdout ----</span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 4</span>
<span class="line"> right: 3</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p><span>Which </span>&hellip;<span> is something we got like 5% into this article already, with normal threads! But there</span>&rsquo;<span>s</span>
<span>more to this failure. First, it is reproducible. If I specify the same seed, I get the </span><em><span>exact</span></em><span> same</span>
<span>interleaving:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x71aafcd900000020</span>);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And this is completely machine independent! If </span><em><span>you</span></em><span> specify this seed, you</span>&rsquo;<span>ll get exact same</span>
<span>interleaving. So, if I am having trouble debugging this, I can DM you this hex in Zulip, and</span>
<span>you</span>&rsquo;<span>ll be able to help out!</span></p>
<p><span>But there</span>&rsquo;<span>s more </span>&mdash;<span> we don</span>&rsquo;<span>t need to debug this failure, we can minimize it!</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x71aafcd900000020</span>)</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">minimize</span>();</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>This gives me the following minimization trace:</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">seed 0x4fd7ddff00000020, seed size 32, search time 106.00ns</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: increment</span>
<span class="line">seed 0x540c0c1c00000010, seed size 16, search time 282.16µs</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x084ca71200000008, seed size 8, search time 805.74µs</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x5699b19400000004, seed size 4, search time 1.44ms</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x4bb0ea5c00000002, seed size 2, search time 4.03ms</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x9c2a13a600000001, seed size 1, search time 4.31ms</span>
<span class="line"></span>
<span class="line">minimized</span>
<span class="line">seed 0x9c2a13a600000001, seed size 1, search time 100.03ms</span></code></pre>

</figure>
<p><span>That is, we ended up with this tiny, minimal example:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x9c2a13a600000001</span>);</span>
<span class="line">}</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span></code></pre>

</figure>
<p><span>And </span><em><span>this</span></em><span> is how you properly test concurrent data structures.</span></p>
</section>
<section id="Postscript">

    <h2>
    <a href="#Postscript"><span>Postscript</span> </a>
    </h2>
<p><span>Of course, this is just a toy. But you can see some ways to extend it. For example, right now our</span>
<code>AtomicU32</code><span> just delegates to the real one. But what you </span><em><span>could</span></em><span> do instead is, for each atomic, to</span>
<span>maintain a set of values written and, on read, return an </span><em><span>arbitrary</span></em><span> written value consistent with a</span>
<span>weak memory model.</span></p>
<p><span>You could also be smarter with exploring interleavings. Instead of interleaving threads at random,</span>
<span>like we do here, you can try to apply model checking approaches and prove that you have considered</span>
<span>all meaningfully different interleavings.</span></p>
<p><span>Or you can apply the approach from </span><a href="https://matklad.github.io/2021/11/07/generate-all-the-things.html"><em><span>Generate All The</span>
<span>Things</span></em></a><span> and exhaustively</span>
<span>enumerate </span><em><span>all</span></em><span> interleavings for up to, say, five increments. In fact, why don</span>&rsquo;<span>t we just do this?</span></p>
<p><code class="display">$ cargo add exhaustigen</code></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">exhaustytest</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">g</span> = exhaustigen::Gen::<span class="hl-title function_ invoke__">new</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">interleavings_count</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">while</span> !g.<span class="hl-title function_ invoke__">done</span>() {</span>
<span class="line">    interleavings_count += <span class="hl-number">1</span>;</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">increment_count</span> = g.<span class="hl-title function_ invoke__">gen</span>(<span class="hl-number">5</span>) <span class="hl-keyword">as</span> <span class="hl-type">u32</span>;</span>
<span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line"></span>
<span class="line">      <span class="hl-symbol">&#x27;outer</span>: <span class="hl-keyword">while</span> t1.<span class="hl-title function_ invoke__">is_paused</span>()</span>
<span class="line">        || t2.<span class="hl-title function_ invoke__">is_paused</span>()</span>
<span class="line">        || counter_model &lt; increment_count</span>
<span class="line">      {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> [&amp;t1, &amp;t2] {</span>
<span class="line">          <span class="hl-keyword">if</span> g.<span class="hl-title function_ invoke__">flip</span>() {</span>
<span class="line">            <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">              t.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">              <span class="hl-keyword">continue</span> <span class="hl-symbol">&#x27;outer</span>;</span>
<span class="line">            }</span>
<span class="line">            <span class="hl-keyword">if</span> counter_model &lt; increment_count {</span>
<span class="line">              t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">              counter_model += <span class="hl-number">1</span>;</span>
<span class="line">              <span class="hl-keyword">continue</span> <span class="hl-symbol">&#x27;outer</span>;</span>
<span class="line">            }</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> [t1, t2] {</span>
<span class="line">          t.<span class="hl-title function_ invoke__">join</span>()</span>
<span class="line">        };</span>
<span class="line">      }</span>
<span class="line"></span>
<span class="line">      <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line">    });</span>
<span class="line">  }</span>
<span class="line">  eprintln!(<span class="hl-string">&quot;interleavings_count = {:?}&quot;</span>, interleavings_count);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The shape of the test is more or less the same, except that we need to make sure that there are no</span>
&ldquo;<span>dummy</span>&rdquo;<span> iterations, and that we always either unpause a thread or submit an increment.</span></p>
<p><span>It finds the same bug, naturally:</span></p>

<figure class="code-block">


<pre><code><span class="line">thread 'exhaustytest' panicked at src/lib.rs:103:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 2</span>
<span class="line"> right: 1</span></code></pre>

</figure>
<p><span>But the cool thing is, if we fix the issue by using atomic increment, </span>&hellip;</p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">fetch_add</span>(</span>
<span class="line">    &amp;<span class="hl-keyword">self</span>,</span>
<span class="line">    value: <span class="hl-type">u32</span>,</span>
<span class="line">    ordering: Ordering,</span>
<span class="line">  ) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">fetch_add</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">fetch_add</span>(<span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p>&hellip;<span> we can get a rather specific correctness statements out of our test, that </span><em><span>any</span></em><span> sequence of at</span>
<span>most five increments is correct:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> t cargo t -r -- exhaustytest --nocapture</span>
<span class="line"><span class="hl-output">running 1 test</span></span>
<span class="line"><span class="hl-output">all 81133 interleavings are fine!</span></span>
<span class="line"><span class="hl-output">test exhaustytest ... ok</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-output">real 8.65s</span></span>
<span class="line"><span class="hl-output">cpu  8.16s (2.22s user + 5.94s sys)</span></span>
<span class="line"><span class="hl-output">rss  63.91mb</span></span></code></pre>

</figure>
<p><span>And the last small thing. Recall that our PBT minimized the first sequence it found </span>&hellip;<span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 4</span>
<span class="line"> right: 3</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p>&hellip;<span> down to just</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:57:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 2</span>
<span class="line"> right: 1</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x9c2a13a600000001</span></code></pre>

</figure>
<p><span>But we never implemented shrinking! How is this possible? Well, strictly speaking, this is out of</span>
<span>scope for this post. And I</span>&rsquo;<span>ve already described this</span>
<a href="https://tigerbeetle.com/blog/2023-03-28-random-fuzzy-thoughts"><span>elsewhere</span></a><span>. And, at 32k, this is the</span>
<span>third-longest post on this blog. And it</span>&rsquo;<span>s 3AM here in Lisbon right now. But of course I</span>&rsquo;<span>ll explain!</span></p>
<p><span>The trick is the simplified </span><a href="https://hypothesis.works/articles/compositional-shrinking/"><span>hypothesis</span>
<span>approach</span></a><span>. The</span>
<a href="https://docs.rs/arbtest/latest/arbtest/"><span>arbtest</span></a><span> PBT library we use in this post is based on a</span>
<span>familiar interface of a PRNG:</span></p>

<figure class="code-block">


<pre><code><span class="line">arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">random_int</span>: <span class="hl-type">usize</span> = rng.<span class="hl-title function_ invoke__">int_in_range</span>(<span class="hl-number">0</span>..=<span class="hl-number">100</span>)?;</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">random_bool</span>: <span class="hl-type">bool</span> = rng.<span class="hl-title function_ invoke__">arbitrary</span>()?;</span>
<span class="line">  <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">});</span></code></pre>

</figure>
<p><span>But there</span>&rsquo;<span>s a twist! This is a </span><em><span>finite</span></em><span> PRNG. So, if you ask it to flip a coin it can give you</span>
<span>heads. And next time it might give you tails. But if you continue asking it for more, at some point</span>
<span>it</span>&rsquo;<span>ll give you </span><span class="display"><code>Err(OutOfEntropy)</code><span>.</span></span></p>
<p><span>That</span>&rsquo;<span>s why all these </span><code>?</code><span> and the outer loop of</span>
<span class="display"><code>while !rng.is_empty() {</code><span>.</span></span></p>
<p><span>In other words, as soon as the test runs out of entropy, it short-circuits and completes. And that</span>
<span>means that by reducing the amount of entropy available the test becomes shorter, and this works</span>
<span>irrespective of how complex is the logic inside the test!</span></p>
<p><span>And </span>&ldquo;<span>entropy</span>&rdquo;<span> is a big scary word here, what actually happens is that the PRNG is just an </span><code>&amp;mut
&amp;[u8]</code><span> inside. That is, a slice of random bytes, which is shortened every time you ask for a random</span>
<span>number. And the shorter the initial slice, the simpler the test gets. Minimization can be this</span>
<span>simple!</span></p>
<p><span>You can find source code for this article at</span>
<a href="https://github.com/matklad/properly-concurrent" class="display url">https://github.com/matklad/properly-concurrent</a></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Regular, Recursive, Restricted</title>
<link href="https://matklad.github.io/2024/06/04/regular-recursive-restricted.html" rel="alternate" type="text/html" title="Regular, Recursive, Restricted" />
<published>2024-06-04T00:00:00+00:00</published>
<updated>2024-06-04T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/06/04/regular-recursive-restricted</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A post/question about formal grammars, wherein I search for a good formalism for describing infix
expressions.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/06/04/regular-recursive-restricted.html"><![CDATA[
<h1><span>Regular, Recursive, Restricted</span> <time class="meta" datetime="2024-06-04">Jun 4, 2024</time></h1>
<p><span>A post/question about formal grammars, wherein I search for a good formalism for describing infix</span>
<span>expressions.</span></p>
<p><span>Problem statement: it</span>&rsquo;<span>s hard to describe arithmetic expressions in a way that:</span></p>
<ul>
<li>
<span>declaratively captures the overall shape of expression, and</span>
</li>
<li>
<span>has a clear precedence semantics</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with the following grammar for arithmetic expressions:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' Expr ')'</span>
<span class="line">  | Expr '+' Expr</span>
<span class="line">  | Expr '*' Expr</span></code></pre>

</figure>
<p><span>It is definitely declarative and obvious. But it is ambiguous </span>&mdash;<span> it doesn</span>&rsquo;<span>t tell whether </span><code>*</code><span> or </span><code>+</code>
<span>binds tighter, and their associativity. You </span><em><span>can</span></em><span> express those properties directly in the grammar:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    Factor</span>
<span class="line">  | Expr '+' Factor</span>
<span class="line"></span>
<span class="line">Factor =</span>
<span class="line">    Atom</span>
<span class="line">  | Factor '*' Atom</span>
<span class="line"></span>
<span class="line">Atom = 'number' | '(' Expr ')'</span></code></pre>

</figure>
<p><span>But at this point we lose decorativeness. The way my brain parses the above grammar is by pattern</span>
<span>matching it as a grammar for infix expressions and folding it back to the initial compressed form,</span>
<em><span>not</span></em><span> by reading the grammar rules as written.</span></p>
<p><span>To go in another direction, you can define ambiguity away and get parsing expression grammars:</span></p>

<figure class="code-block">


<pre><code><span class="line">Exp =</span>
<span class="line">    Sum</span>
<span class="line">  / Product</span>
<span class="line">  / Atom</span>
<span class="line"></span>
<span class="line">Sum     = Expr (('+' / '-') Expr)+</span>
<span class="line">Product = Expr (('*' / '/') Expr)+</span>
<span class="line"></span>
<span class="line">Atom = 'number' | '(' Expr ')'</span></code></pre>

</figure>
<p><span>This captures precedence </span><em><span>mostly</span></em><span> declaratively: we first match </span><code>Sum</code><span>, and, failing that, match</span>
<code>Product</code><span>. But the clarity of semantics is lost </span>&mdash;<span> PEGs are never ambiguous by virtue of always</span>
<span>picking the first alternative, so it</span>&rsquo;<span>s too easy to introduce an unintended ambiguity.</span></p>
<p><span>Can we have both? Clarity with respect to tree shape and clarity with respect to ambiguity?</span></p>
<p><span>Let me present a formalism that, I think, ticks both boxes for the toy example and pose a question</span>
<span>of whether it generalizes.</span></p>
<hr>
<p><span>Running example:</span></p>

<figure class="code-block">


<pre><code><span class="line">E =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' E ')'</span>
<span class="line">  | E '+' E</span></code></pre>

</figure>
<p><span>As a grammar for strings, it is ambiguous. There are two parse trees for </span><code>1 + 2 + 3</code><span> </span>&mdash;<span> the</span>
&ldquo;<span>correct</span>&rdquo;<span> one </span><code>(1 + 2) + 3</code><span>, and the alternative: </span><code>1 + (2 + 3)</code><span>.</span></p>
<p><span>Instead, lets see it as a grammar for trees instead. Specifically, trees where:</span></p>
<ul>
<li>
<span>Leaves are labeled with </span><code>'number'</code><span>, </span><code>'+'</code><span>, or </span><code>'*'</code><span>.</span>
</li>
<li>
<span>Interior nodes are labeled with </span><code>E</code><span>.</span>
</li>
<li>
<span>For each interior node, the string formed by labels of its </span><em><span>direct</span></em><span> children conforms to the</span>
<span>specified regular expression.</span>
</li>
</ul>
<p><span>For trees, this is a perfectly fine grammar! Given a labeled tree, its trivial to check whether it</span>
<span>matches the grammar: for each node, you can directly match the regular expression. There</span>&rsquo;<span>s also no</span>
<span>meaningful ambiguity </span>&mdash;<span> while arbitrary regular expressions can be ambiguous (</span><code>aa | a*</code><span>), this</span>
<span>doesn</span>&rsquo;<span>t really come up as harmful in practice all that often, and, in any case, it</span>&rsquo;<span>s easy to check</span>
<span>that any two regular alternatives are disjoint (intersect the two automata, minimize the result,</span>
<span>check if it is empty).</span></p>
<p><span>As a grammar for trees, it has the following property: there are two distinct trees which</span>
<span>nevertheless share the same sequence of leaves:</span></p>

<figure class="code-block">


<pre><code><span class="line">        E                  E</span>
<span class="line">        o                  o</span>
<span class="line">      / | \              / | \</span>
<span class="line">     E '+' E            E '+' E</span>
<span class="line">     o     |            |     o</span>
<span class="line">   / | \  '3'          '1'  / | \</span>
<span class="line">  E '+' E                  E '+' E</span>
<span class="line">  |     |                  |     |</span>
<span class="line"> '1'   '2'                '2'   '3'</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s restrict the set of trees, in the most straightforward manner, by adding some inequalities:</span></p>

<figure class="code-block">


<pre><code><span class="line">E =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' E ')'</span>
<span class="line">  | E '+' E</span>
<span class="line"></span>
<span class="line">E !=</span>
<span class="line">    E '+' [E '+' E]</span></code></pre>

</figure>
<p><span>Here, square brackets denote a child. </span><code>E '+' [E '+' E]</code><span> is a plus node whose right child is also a</span>
<span>plus node. Checking whether a tree conform to this modified set of rules is easy as negative rules</span>
<span>are also just regular expressions. Well, I think you need some fiddling here, as, as written, a</span>
<span>negative rule matches two different levels of the tree, but you can flatten both the rule and the</span>
<span>actual tree to the grandchildren level by enclosing children in parenthesis. Let me show an example:</span></p>
<p><span>We want to match this node:</span></p>

<figure class="code-block">


<pre><code><span class="line">    E</span>
<span class="line">    o</span>
<span class="line">  / | \</span>
<span class="line"> E '+' E</span>
<span class="line"> |     o</span>
<span class="line">'1'  / | \</span>
<span class="line">    E '+' E</span></code></pre>

</figure>
<p><span>against this rule concerning children and grand children:</span></p>

<figure class="code-block">


<pre><code><span class="line">E '+' [E '+' E]</span></code></pre>

</figure>
<p><span>We write the list of children and grandchidren of the node, while adding extra </span><code>[]</code><span>, to get this</span>
<span>string:</span></p>

<figure class="code-block">


<pre><code><span class="line">['1'] '+' [E '+' E]</span></code></pre>

</figure>
<p><span>And in the rule we replace top-level non-terminals with </span><code>[.*]</code><span>, to get this regular expression:</span></p>

<figure class="code-block">


<pre><code><span class="line">[.*] '+' [E '+' E]</span></code></pre>

</figure>
<p><span>Now we can match the string against a regex, get a mach, and rule out the tree (remember, this is</span>
<code>!=</code><span>).</span></p>
<p><span>So here it is, a perfectly functional mathematical animal: recursive restricted regular expression:</span></p>
<ul>
<li>
<span>A set of non-terminals </span><code>N</code><span> (denoted with </span><code>TitleCase</code><span> names)</span>
</li>
<li>
<span>A set of terminals </span><code>T</code><span> (denoted with </span><code>'quoted'</code><span> names)</span>
</li>
<li>
<span>A generating mapping from non-terminals </span><code>N</code><span> to regular expressions over </span><code>N ∪ T</code><span> alphabet</span>
</li>
<li>
<span>A restricting mapping from non-terminals </span><code>N</code><span> to regular expressions over </span><code>N ∪ T ∪ {], [}</code><span> (that is</span>
<span>regular expressions with square brackets to denote children)</span>
</li>
</ul>
<p><span>This construction denotes a set of labeled trees, where interior nodes are labeled with </span><code>N</code><span>, leaves</span>
<span>are labeled with </span><code>T</code><span> and for each interior node</span></p>
<ul>
<li>
<span>its children match the corresponding generating regular expression</span>
</li>
<li>
<span>its grandchildren do not match the corresponding restricting regular expression</span>
</li>
</ul>
<p><span>And the main question one would have, if confronted with a specimen, is </span>&ldquo;<span>is it ambiguous?</span>&rdquo;<span> That is,</span>
<span>are there two trees in the set which have the same sequence of leaves?</span></p>
<p><span>Let</span>&rsquo;<span>s look at an example:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' Expr ')'</span>
<span class="line">  | Expr '+' Expr</span>
<span class="line">  | Expr '*' Expr</span>
<span class="line"></span>
<span class="line">Expr !=</span>
<span class="line">             Expr '+' [Expr '+' Expr]</span>
<span class="line">|            Expr '*' [Expr '*' Expr]</span>
<span class="line">|            Expr '*' [Expr '+' Expr]</span>
<span class="line">| [Expr '+' Expr] '*' Expr</span></code></pre>

</figure>
<p><span>It looks unambiguous to me! And I am pretty sure that I can prove, by hand, that it is in fact</span>
<span>unambiguous (well, I might discover that I miss a couple of restrictions in process, but it feels</span>
<span>like it should work in principle). The question is, can a computer take an arbitrary recursive</span>
<span>restricted regular expression and tell me that its unambiguous, or, failing that, provide a</span>
<span>counter-example?</span></p>
<p><span>In the general case, the answer is no </span>&mdash;<span> this is at least as expressive as CFG, and ambiguity of</span>
<span>arbitrary CFG is undecidable. But perhaps there</span>&rsquo;<span>s some reasonable set of restrictions under which it</span>
<span>is in fact possible to prove the absence of ambiguity?</span></p>
]]></content>
</entry>

<entry>
<title type="text">Basic Things</title>
<link href="https://matklad.github.io/2024/03/22/basic-things.html" rel="alternate" type="text/html" title="Basic Things" />
<published>2024-03-22T00:00:00+00:00</published>
<updated>2024-03-22T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/03/22/basic-things</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[After working on the initial stages of several largish projects, I accumulated a list of things that
share the following three properties:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/03/22/basic-things.html"><![CDATA[
<h1><span>Basic Things</span> <time class="meta" datetime="2024-03-22">Mar 22, 2024</time></h1>
<p><span>After working on the initial stages of several largish projects, I accumulated a list of things that</span>
<span>share the following three properties:</span></p>
<ul>
<li>
<span>they are irrelevant while the project is small,</span>
</li>
<li>
<span>they are a productivity multiplier when the project is large,</span>
</li>
<li>
<span>they are much harder to introduce down the line.</span>
</li>
</ul>
<p><span>Here</span>&rsquo;<span>s the list:</span></p>
<section id="READMEs">

    <h2>
    <a href="#READMEs"><span>READMEs</span> </a>
    </h2>
<p><span>A project should have a </span><em><span>short</span></em><span> one-page readme that is mostly links to more topical documentation.</span>
<span>The two most important links are the user docs and the dev docs.</span></p>
<p><span>A common failure is a readme growing haphazardly by accretion, such that it is neither a good</span>
<span>landing page, nor a source of comprehensive docs on any particular topic. It is hard to refactor</span>
<span>such an unstructured readme later. The information is valuable, if disorganized, but there</span>
<span>isn</span>&rsquo;<span>t any better place to move it to.</span></p>
</section>
<section id="Developer-Docs">

    <h2>
    <a href="#Developer-Docs"><span>Developer Docs</span> </a>
    </h2>
<p><span>For developers, you generally want to have a docs folder in the repository. The docs folder should</span>
<em><span>also</span></em><span> contain a short landing page describing the structure of the documentation. This structure</span>
<span>should allow for both a small number of high quality curated documents, and a large number of ad-hoc</span>
<span>append-only notes on any particular topic. For example, </span><code>docs/README.md</code><span> could point to carefully</span>
<span>crafted</span>
<a href="https://matklad.github.io/2021/02/06/ARCHITECTURE.md.html"><code>ARCHITECTURE.md</code></a>
<span>and </span><code>CONTRIBUTING.md</code><span>, which describe high level code and social</span>
<span>architectures, and explicitly say that everything else in the </span><code>docs/</code><span> folder is a set of unorganized</span>
<span>topical guides.</span></p>
<p><span>Common failure modes here:</span></p>
<ol type="a">
<li>
<p><span>There</span>&rsquo;<span>s no place where to put new developer documentation at all. As a result, no docs are</span>
<span>getting written, and, by the time you do need docs, the knowledge is lost.</span></p>
</li>
<li>
<p><span>There</span>&rsquo;<span>s only highly  structured, carefully reviewed developer documentation. Contributing docs</span>
<span>requires a lot of efforts, and many small things go undocumented.</span></p>
</li>
<li>
<p><span>There</span>&rsquo;<span>s only unstructured append-only pile of isolated documents. Things are </span><em><span>mostly</span></em><span> documented,</span>
<span>often two or three times, but any new team member has to do the wheat from the chaff thing anew.</span></p>
</li>
</ol>
</section>
<section id="Users-Website">

    <h2>
    <a href="#Users-Website"><span>Users Website</span> </a>
    </h2>
<p><span>Most project can benefit from a dedicated website targeted at the users. You want to have website</span>
<span>ready when there are few-to-no users: usage compounds over time, so, if you find yourself with a</span>
<span>significant number of users and no web </span>&ldquo;<span>face</span>&rdquo;<span>, you</span>&rsquo;<span>ve lost quite a bit of value already!</span></p>
<p><span>Some other failure modes here:</span></p>
<ol type="a">
<li>
<p><span>A different team manages the website. This prevents project developers from directly contributing</span>
<span>improvements, and may lead to divergence between the docs and the shipped product.</span></p>
</li>
<li>
<p><span>Today</span>&rsquo;<span>s web stacks gravitate towards infinite complexity. It</span>&rsquo;<span>s all too natural to pick an </span>&ldquo;<span>easy</span>&rdquo;
<span>heavy framework at the start, and then get yourself into npm</span>&rsquo;<span>s bog. Website is about content, and</span>
<span>content has gravity. Whatever markup language dialect you choose at the beginning is going to</span>
<span>stay with for some time. Do carefully consider the choice of your web stack.</span></p>
</li>
<li>
<p><span>Saying that which isn</span>&rsquo;<span>t quite done yet. Don</span>&rsquo;<span>t overpromise, it</span>&rsquo;<span>s much easier to say more later</span>
<span>than to take back your words, and humbleness might be a good marketing. Consider if you are in a</span>
<span>domain where engineering credibility travel faster than buzz words. But this is situational. More</span>
<span>general advice would be that marketing also compounds over time, so it pays off to be deliberate</span>
<span>about your image from the start.</span></p>
</li>
</ol>
</section>
<section id="Internal-Website">

    <h2>
    <a href="#Internal-Website"><span>Internal Website</span> </a>
    </h2>
<p><span>This is more situational, but consider if, in addition to public-facing website, you also need an</span>
<span>internal, engineering-facing one. At some point you</span>&rsquo;<span>ll probably need a bit more interactivity than</span>
<span>what</span>&rsquo;<span>s available in a </span><code>README.md</code><span> </span>&mdash;<span> perhaps you need a place to display code-related metrics like</span>
<span>coverage or some javascript to compute release rotation. Having a place on the web where a</span>
<span>contributor can place something they need right now without much red tape is nice!</span></p>
<p><span>This is a recurring theme </span>&mdash;<span> you should be organized, you should not be organized. </span><em><span>Some</span></em><span> things</span>
<span>have large fan-out and should be guarded with careful review. </span><em><span>Other</span></em><span> things benefit from just being</span>
<span>there and a lightweight process. You need to create places for both kinds of things, and a clear</span>
<span>decision rule about what goes where.</span></p>
<p><span>For internal website, you</span>&rsquo;<span>ll probably need some kind of data store as well. If you want to track</span>
<span>binary size across commits, </span><em><span>something</span></em><span> needs to map commit hashes to (lets be optimistic)</span>
<span>kilobytes! I don</span>&rsquo;<span>t know a good solution here. I use a JSON file in a github repository for similar</span>
<span>purposes.</span></p>
</section>
<section id="Process-Docs">

    <h2>
    <a href="#Process-Docs"><span>Process Docs</span> </a>
    </h2>
<p><span>There are many possible ways to get some code into the main branch. Pick one, and spell it out in</span>
<span>an </span><code>.md</code><span> file explicitly:</span></p>
<ul>
<li>
<p><span>Are feature branches pushed to the central repository, or is anyone works off their fork? I find</span>
<span>forks work better in general as they automatically namespace everyone</span>&rsquo;<span>s branches, and put team</span>
<span>members and external contributors on equal footing.</span></p>
</li>
<li>
<p><span>If the repository is shared, what is the naming convention for branches? I prefix mine with</span>
<code>matklad/</code><span>.</span></p>
</li>
<li>
<p><span>You use </span><a href="https://graydon2.dreamwidth.org/1597.html"><span>not rocket-science rule</span></a><span> (more on this later :).</span></p>
</li>
<li>
<p><span>Who should do code review of a particular PR? A single person, to avoid bystander effect and to</span>
<span>reduce notification fatigue. The reviewer is picked by the author of PR, as that</span>&rsquo;<span>s a stable</span>
<span>equilibrium in a high-trust team and cuts red tape.</span></p>
</li>
<li>
<p><span>How the reviewer knows that they need to review code? On GitHub, you want to </span><em><span>assign</span></em><span> rather than</span>
<em><span>request</span></em><span> a review. Assign is level-triggered </span>&mdash;<span> it won</span>&rsquo;<span>t go away until the PR is merged, and it</span>
<span>becomes the responsibility of the reviewer to help the PR along until it is merged (</span><em><span>request</span>
<span>review</span></em><span> is still useful to poke the assignee after a round of feedback&amp;changes). More generally,</span>
<span>code review is the highest priority task </span>&mdash;<span> there</span>&rsquo;<span>s no reason to work on new code</span>
<span>if there</span>&rsquo;<span>s already some finished code which is just blocked on your review.</span></p>
</li>
<li>
<p><span>What is the purpose of review? Reviewing for correctness, for single voice, for idioms, for</span>
<span>knowledge sharing, for high-level architecture are choices! Explicitly spell out what makes most</span>
<span>sense in the context of your project.</span></p>
</li>
<li>
<p><span>Meta process docs: positively encourage contributing process documentation itself.</span></p>
</li>
</ul>
</section>
<section id="Style">

    <h2>
    <a href="#Style"><span>Style</span> </a>
    </h2>
<p><span>Speaking about meta process, style guide is where it is most practically valuable. Make sure that</span>
<span>most stylistic comments during code reviews are immediately codified in the project-specific style</span>
<span>document. New contributors should learn project</span>&rsquo;<span>s voice not through a hundred repetitive comments on</span>
<span>PRs, but through a dozen links to specific items of the style guide.</span></p>
<p><span>Do you even need a project-specific style guide? I think you do </span>&mdash;<span> cutting down mental energy for</span>
<span>trivial decisions is helpful. If you need a result variable, and half of the functions call it </span><code>res</code>
<span>and another half of the functions call it </span><code>result</code><span>, making this choice is just distracting.</span></p>
<p><span>Project-specific naming conventions is one of the more useful thing to place in the style guide.</span></p>
<p><span>Optimize style guide for extensibility. Uplifting a comment from a code review to the style guide</span>
<span>should not require much work.</span></p>
<p><span>Ensure that there</span>&rsquo;<span>s a style tzar </span>&mdash;<span> building consensus around </span><em><span>specific</span></em><span> style choices is very</span>
<span>hard, better to delegate the entire responsibility to one person who can make good enough choices.</span>
<span>Style usually is not about what</span>&rsquo;<span>s better, it</span>&rsquo;<span>s about removing needless options in a semi-arbitrary</span>
<span>ways.</span></p>
</section>
<section id="Git">

    <h2>
    <a href="#Git"><span>Git</span> </a>
    </h2>
<p><span>Document stylistic details pertaining to git. If project uses </span><code>area:</code><span> prefixes for commits, spell</span>
<span>out an explicit list of such prefixes.</span></p>
<p><span>Consider documenting acceptable line length for the summary line. Git man page boldly declares that</span>
<span>a summary should be under 50 characters, but that is just plain false. Even in the kernel, most</span>
<span>summaries are somewhere between 50 and 80 characters.</span></p>
<p><span>Definitely explicitly forbid adding large files to git. Repository size increases monotonically,</span>
<code>git clone</code><span> time is important.</span></p>
<p><span>Document merge-vs-rebase thing. My preferred answer is:</span></p>
<ul>
<li>
<span>A unit of change is a pull request, which might contain several commits</span>
</li>
<li>
<span>Merge commit for the pull request is what is being tested</span>
</li>
<li>
<span>The main branch contains only merge commits</span>
</li>
<li>
<span>Conversely, </span><em><span>only</span></em><span> the main branch contains merge commits, pull requests themselves are always</span>
<span>rebased.</span>
</li>
</ul>
<p><span>Forbidding large files in the repo is a good policy, but it</span>&rsquo;<span>s hard to follow. Over the lifetime of</span>
<span>the project, someone somewhere will sneakily add and revert a megabyte of generated protobufs, and</span>
<span>that will fly under code review radar.</span></p>
<p><span>This brings us to the most basic thing of them all:</span></p>
</section>
<section id="Not-Rocket-Science-Rule">

    <h2>
    <a href="#Not-Rocket-Science-Rule"><span>Not Rocket Science Rule</span> </a>
    </h2>
<p><span>Maintain a well-defined set of automated checks that pass on the main branch at all times. If you</span>
<span>don</span>&rsquo;<span>t want large blobs in git repository, write a test rejecting large git objects and run that</span>
<span>right before updating the main branch. No merge commits on feature branches? Write a test which</span>
<span>fails with a pageful of Git self-help if one is detected. Want to wrap </span><code>.md</code><span> at 80 columns? Write a</span>
<span>test :)</span></p>
<p><span>It is perhaps worth you while to re-read the original post:</span>
<a href="https://graydon2.dreamwidth.org/1597.html" class="display url">https://graydon2.dreamwidth.org/1597.html</a></p>
<p><a href="https://matklad.github.io/2024/01/03/of-rats-and-ratchets.html"><span>This mindset of monotonically growing set of properties</span></a>
<span>that are true about the codebase is </span><em><span>incredibly</span></em><span> powerful. You start seeing code as temporary, fluid</span>
<span>thing that can always be changed relatively cheaply, and the accumulated set of automated tests as</span>
<span>the real value of the project.</span></p>
<p><span>Another second order effect is that NRSR puts a pressure to optimize your build and test</span>
<span>infrastructure. If you don</span>&rsquo;<span>t have an option to merge the code when an unrelated flaky test fails,</span>
<span>you won</span>&rsquo;<span>t have flaky tests.</span></p>
<p><span>A common anti-pattern here is that a project grows a set of semi-checks </span>&mdash;<span> tests that exists, but</span>
<span>are not 100% reliable, and thus are not exercised by the CI routinely. And that creates ambiguity</span>
&mdash;<span> are tests failing due to a regression which should be fixed, or were they never reliable, and</span>
<span>just test a property that isn</span>&rsquo;<span>t actually essential for functioning of the project? This fuzziness</span>
<span>compounds over time. If a check isn</span>&rsquo;<span>t reliable enough to be part of NRSR CI gate, it isn</span>&rsquo;<span>t actually</span>
<span>a check you care about, and should be removed.</span></p>
<p><span>But to do NRSR, you need to build &amp; CI your code first:</span></p>
</section>
<section id="Build-CI">

    <h2>
    <a href="#Build-CI"><span>Build &amp; CI</span> </a>
    </h2>
<p><span>This is a complex topic. Let</span>&rsquo;<span>s start with the basics: what is a build system? I would love to</span>
<span>highlight a couple of slightly unconventional answers here.</span></p>
<p><em><span>First</span></em><span>, a build system is a bootstrap process: it is how you get from </span><code>git clone</code><span> to a working</span>
<span>binary. The two aspects of this boostrapping process are important:</span></p>
<ul>
<li>
<span>It should be simple. No</span>
<span class="display"><code>sudo apt-get install bazzilion packages</code><span>,</span></span>
<span>the single binary of your build system should be able to bring everything else that</span>&rsquo;<span>s needed,</span>
<span>automatically.</span>
</li>
<li>
<span>It should be repeatable. Your laptop and your CI should end up with exactly identical set of</span>
<span>dependencies. The end result should be a function of commit hash, and not your local shell</span>
<span>history, otherwise NRSR doesn</span>&rsquo;<span>t work.</span>
</li>
</ul>
<p><em><span>Second</span></em><span>, a build system is developer UI. To do almost anything, you need to type some sort of build</span>
<span>system invocation into your shell. There should be a single, clearly documented command for building</span>
<span>and testing the project. If it is not a single </span><code>makebelieve test</code><span>, something</span>&rsquo;<span>s wrong.</span></p>
<p><span>One anti-pattern here is when the build system spills over to CI. When, to figure out what the set</span>
<span>of checks even is, you need to read </span><code>.github/workflows/*.yml</code><span> to compile a list of commands. That</span>&rsquo;<span>s</span>
<span>accidental complexity! Sprawling yamls are a bad entry point. Put all the logic into the build</span>
<span>system and let the CI drive that, and not vice verse.</span></p>
<p><a href="https://matklad.github.io/2023/12/31/O(1)-build-file.html"><span>There is a stronger version of the</span>
<span>advice</span></a><span>. No matter the size of the</span>
<span>project, there</span>&rsquo;<span>s probably only a handful of workflows that make sense for it: testing, running,</span>
<span>releasing, etc. This small set of workflows should be nailed from the start, and specific commands</span>
<span>should be documented. When the project subsequently grows in volumes, this set of build-system entry</span>
<span>points should </span><em><span>not</span></em><span> grow.</span></p>
<p><span>If you add a Frobnicator, </span><code>makebelieve test</code><span> invocation </span><em><span>should</span></em><span> test that Frobnicator works. If</span>
<span>instead you need a dedicated </span><code>makebelieve test-frobnicator</code><span> and the corresponding line in some CI</span>
<span>yaml, you are on a perilous path.</span></p>
<p><em><span>Finally</span></em><span>, a build system is a collection of commands to make stuff happen. In larger projects,</span>
<span>you</span>&rsquo;<span>ll inevitably need some non-trivial amount of glue automation. Even if the entry point is just</span>
<code>makebelive release</code><span>, internally that might require any number of different tools to build, sign,</span>
<span>tag, upload, validate, and generate a changelog for a new release.</span></p>
<p><span>A common anti-pattern is to write these sorts of automations in bash and Python, but that</span>&rsquo;<span>s almost</span>
<span>pure technical debt. These ecosystems are extremely finnicky in and off themselves, and, crucially</span>
<span>(unless your project itself is written in bash or Python), they are a second ecosystem to what you</span>
<span>already have in your project for </span>&ldquo;<span>normal</span>&rdquo;<span> code.</span></p>
<p><span>But releasing software is also just code, which you can write in your primarly language.</span>
<a href="https://twitter.com/id_aa_carmack/status/989951283900514304"><span>The right tool for the job is often the tool you are already using</span></a><span>.</span>
<span>It pays off to explicitly attack the problem of glue from the start, and to pick/write a library</span>
<span>that makes writing subprocess wrangling logic easy.</span></p>
<p><span>Summing the build and CI story up:</span></p>
<p><span>Build system is self-contained, reproducible and takes on the task of downloading all external</span>
<span>dependencies. Irrespective of size of the project, it contains O(1) different entry points. One of</span>
<span>those entry points is triggered by the not rocket science rule CI infra to run the set of canonical</span>
<span>checks. There</span>&rsquo;<span>s an explicit support for free-form automation, which is implemented in the same</span>
<span>language as the bulk of the project.</span></p>
<p><span>Integration with NRSR is the most important aspect of the build process, as it determines how the</span>
<span>project evolves over time. Let</span>&rsquo;<span>s zoom in.</span></p>
</section>
<section id="Testing">

    <h2>
    <a href="#Testing"><span>Testing</span> </a>
    </h2>
<p><span>Testing is a primary architectural concern. When the first line of code is written, you already</span>
<span>should understand the big picture testing story. It is empathically </span><em><span>not</span></em><span> </span>&ldquo;<span>every class and module</span>
<span>has unit-test</span>&rdquo;<span>. Testing should be data oriented </span>&mdash;<span> the job of a particular software is to take some</span>
<span>data in, transform it, and spit different data out. Overall testing strategy requires:</span></p>
<ul>
<li>
<span>some way to specify/generate input data,</span>
</li>
<li>
<span>some way to assert desired properties of output data, and</span>
</li>
<li>
<span>a way to run many individual checks very fast.</span>
</li>
</ul>
<p><span>If time is a meaningful part of the input data, it should be modeled explicitly. Not getting the</span>
<span>testing architecture right usually results in:</span></p>
<ul>
<li>
<span>Software that is hard to change because thousands of test nail existing internal APIs.</span>
</li>
<li>
<span>Software that is hard to change because there are no test to confidently verify absence of</span>
<span>unintended breakages.</span>
</li>
<li>
<span>Software that is hard to change because each change requires hours of testing time to verify.</span>
</li>
</ul>
<p><span>How to architect a test suite goes beyond the scope of this article, but please read</span>
<a href="https://matklad.github.io/2022/07/04/unit-and-integration-tests.html"><span>Unit and Integration Tests</span></a>
<span>and</span>
<a href="https://matklad.github.io/2021/05/31/how-to-test.html"><span>How To Test</span></a><span>.</span></p>
<p><span>Some specific things that are in scope for this article:</span></p>
<p><span>Zero tolerance for flaky tests. Strict not rocket science rules gives this by construction </span>&mdash;<span> if</span>
<span>you can</span>&rsquo;<span>t merge </span><em><span>your</span></em><span> pull request because someone elses test is flaky, that flaky test immediately</span>
<span>becomes your problem.</span></p>
<p><span>Fast tests. Again, NRSR already provides a natural pressure for this, but it also helps to make</span>
<span>testing time more salient otherwise. Just by default printing the total test time and five slowest</span>
<span>tests in a run goes a long way.</span></p>
<p><span>Not all tests could be fast. Continuing the ying-yang theme of embracing order and chaos</span>
<span>simultaneously, it helps to introduce the concept of slow tests early on. CI always runs the full</span>
<span>suite of tests, fast and slow. But the local </span><code>makebelive test</code><span> by default runs only fast test, with</span>
<span>an opt-in for slow tests. Opt in can be as simple as an </span><code>SLOW_TESTS=1</code><span> environmental variable.</span></p>
<p><span>Introduce a </span><a href="https://ianthehenry.com/posts/my-kind-of-repl/"><span>snapshot testing</span></a><span> library early.</span>
<span>Although the bulk of tests should probably use project-specific testing harness, for everything else</span>
<span>inline repl-driven snapshot testing is a good default approach, and is something costly to introduce</span>
<span>once you</span>&rsquo;<span>ve accumulated a body of non-snapshot-based tests.</span></p>
<p><span>Alongside the tests, come the benchmarks.</span></p>
</section>
<section id="Benchmarking">

    <h2>
    <a href="#Benchmarking"><span>Benchmarking</span> </a>
    </h2>
<p><span>I don</span>&rsquo;<span>t have a grand vison about how to make benchmark work in a large, living project, it always</span>
<span>feels like a struggle to me. I do have a couple of tactical tips though.</span></p>
<p><em><span>Firstly</span></em><span>, any code that is </span><em><span>not</span></em><span> running during NRSR is effectively dead. It is exceedingly common</span>
<span>for benchmarks to be added alongside a performance improvement, and then </span><em><span>not</span></em><span> getting hooked up</span>
<span>with CI. So, two month down the line, the benchmark either stops compiling outright, or maybe just</span>
<span>panics at a startup due to some unrelated change.</span></p>
<p><span>This fix here is to make sure that every benchmark is </span><em><span>also</span></em><span> a test. Parametrize every benchmark by</span>
<span>input size, such that with a small input it finishes in milliseconds. Then write a test that</span>
<span>literally just calls the benchmarking code with this small input. And remember that your build</span>
<span>system should have O(1) entry points. Plug this into a </span><span class="display"><code>makebelieve test</code><span>,</span></span><span> not into a</span>
<span>dedicated </span><span class="display"><code>makebelieve benchmark --small-size</code><span>.</span></span></p>
<p><em><span>Secondly</span></em><span>, any large project has a certain amount of very important macro metrics.</span></p>
<ul>
<li>
<span>How long does it take to build?</span>
</li>
<li>
<span>How long does it take to test?</span>
</li>
<li>
<span>How large is the resulting artifact shipping to users?</span>
</li>
</ul>
<p><span>These are some of the questions that always matter. You need infrastructure to track these numbers,</span>
<span>and to see them regularly. This where the internal website and its data store come in. During CI,</span>
<span>note those number. After CI run, upload a record with commit hash, metric name, metric value</span>
<em><span>somewhere</span></em><span>. Don</span>&rsquo;<span>t worry if the results are noisy </span>&mdash;<span> you target the baseline here, ability to</span>
<span>notice large changes over time.</span></p>
<p><span>Two options for the </span>&ldquo;<span>upload</span>&rdquo;<span> part:</span></p>
<ul>
<li>
<p><span>Just put them into some </span><code>.json</code><span> file in a git repo, and LLM a bit of javascript to display a nice</span>
<span>graph from these data.</span></p>
</li>
<li>
<p><a href="https://nyrkio.com" class="url">https://nyrkio.com</a><span> is a surprisingly good SaaS offering that I can recommend.</span></p>
</li>
</ul>
</section>
<section id="Fuzz-Testing">

    <h2>
    <a href="#Fuzz-Testing"><span>Fuzz Testing</span> </a>
    </h2>
<p><span>Serious fuzz testing curiously shares characteristics of tests and benchmarks. Like a normal test, a</span>
<span>fuzz test informs you about a correctness issue in your application, and is reproducible. Like a</span>
<span>benchmark, it is (infinitely) long running and infeasible to do as a part of NRSR.</span></p>
<p><span>I don</span>&rsquo;<span>t yet have a good hang on how to most effectively integrate continuous fuzzing into</span>
<span>development process. I don</span>&rsquo;<span>t know what is the not rocket science rule of fuzzing. But two things</span>
<span>help:</span></p>
<p><em><span>First</span></em><span>, even if you can</span>&rsquo;<span>t run fuzzing loop during CI, you can run isolated seeds. To help ensure</span>
<span>that the fuzing code doesn</span>&rsquo;<span>t get broken, do the same thing as with benchmark </span>&mdash;<span> add a test that</span>
<span>runs fuzzing logic with a fixed seed and small, fast parameters. One variation here is that you can</span>
<span>use commit sha as random a seed </span>&mdash;<span> that way the code is still reproducible, but there is enough</span>
<span>variation to avoid dynamically dead code.</span></p>
<p><em><span>Second</span></em><span>, it is helpful to think about fuzzing in terms of level triggering. With tests, when you</span>
<span>make an erroneous commit, you immediately know that it breaks stuff. With fuzzing, you generally</span>
<span>discover this later, and a broken seed generally persists for several commits. So, as an output of</span>
<span>the fuzzer, I think what you want is </span><em><span>not</span></em><span> a set of GitHub issues, but rather a dashboard of sorts</span>
<span>which shows a table of recent commits and failing seeds for those commits.</span></p>
<p><span>With not rocket science rule firmly in place, it makes sense to think about releases.</span></p>
</section>
<section id="Releases">

    <h2>
    <a href="#Releases"><span>Releases</span> </a>
    </h2>
<p><span>Two core insights here:</span></p>
<p><em><span>First</span></em><span> release </span><em><span>process</span></em><span> is orthogonal from software being </span><em><span>production ready</span></em><span>. You can release</span>
<span>stuff before it is ready (provided that you add a short disclaimer to the readme). So, it pays off</span>
<span>to add proper release process early on, such that, when the time comes to actually release</span>
<span>software, it comes down to removing disclaimers and writing the announcement post, as all technical</span>
<span>work has been done ages ago.</span></p>
<p><em><span>Second</span></em><span>, software engineering in general observes reverse triangle inequality: to get from A to C,</span>
<span>it is faster to go from A to B and then from B to C, then moving from A to C atomically. If you make</span>
<span>a pull request, it helps to split it up into smaller parts. If you refactor something, it is faster</span>
<span>to first introduce a new working copy and then separately retire the old code, rather than changing</span>
<span>the thing in place.</span></p>
<p><span>Releases are no different: faster, more frequent releases are easier and less risky. Weekly cadence</span>
<span>works great, provided that you have a solid set of checks in your NRSR.</span></p>
<p><span>It is much easier to start with a state where almost nothing works, but there</span>&rsquo;<span>s a solid release</span>
<span>(with an empty set of features), and ramp up from there, than to hack with reckless abandon</span>
<em><span>without</span></em><span> thinking much about eventual release, and then scramble to decide which is ready and</span>
<span>releasable, a what should be cut.</span></p>
</section>
<section id="Summary">

    <h2>
    <a href="#Summary"><span>Summary</span> </a>
    </h2>
<p><span>I think that</span>&rsquo;<span>s it for today? That</span>&rsquo;<span>s a lot of small points! Here</span>&rsquo;<span>s a bullet list for convenient</span>
<span>reference:</span></p>
<ul>
<li>
<span>README as a landing page.</span>
</li>
<li>
<span>Dev docs.</span>
</li>
<li>
<span>User docs.</span>
</li>
<li>
<span>Structured dev docs (architecture and processes).</span>
</li>
<li>
<span>Unstructured ingest-optimized dev docs (code style, topical guides).</span>
</li>
<li>
<span>User website, beware of content gravity.</span>
</li>
<li>
<span>Ingest-optimized internal web site.</span>
</li>
<li>
<span>Meta documentation process </span>&mdash;<span> its everyone job to append to code style and process docs.</span>
</li>
<li>
<span>Clear code review protocol (in whose court is the ball currently?).</span>
</li>
<li>
<span>Automated check for no large blobs in a git repo.</span>
</li>
<li>
<span>Not rocket science rule.</span>
</li>
<li>
<span>Let</span>&rsquo;<span>s repeat: at </span><strong><span>all</span></strong><span> times, the main branch points at a commit hash which is known to pass a</span>
<span>set of well-defined checks.</span>
</li>
<li>
<span>No semi tests: if the code is not good enough to add to NRSR, it is deleted.</span>
</li>
<li>
<span>No flaky tests (mostly by construction from NRSR).</span>
</li>
<li>
<span>Single command build.</span>
</li>
<li>
<span>Reproducible build.</span>
</li>
<li>
<span>Fixed number of build system entry points. No separate lint step, a lint is a kind of a test.</span>
</li>
<li>
<span>CI delegates to the build system.</span>
</li>
<li>
<span>Space for ad-hoc automation in the main language.</span>
</li>
<li>
<span>Overarching testing infrastructure, grand unified theory of project</span>&rsquo;<span>s testing.</span>
</li>
<li>
<span>Fast/Slow test split (fast=seconds per test suite, slow=low digit minutes per test suite).</span>
</li>
<li>
<span>Snapshot testing.</span>
</li>
<li>
<span>Benchmarks are tests.</span>
</li>
<li>
<span>Macro metrics tracking (time to build, time to test).</span>
</li>
<li>
<span>Fuzz tests are tests.</span>
</li>
<li>
<span>Level-triggered display of continuous fuzzing results.</span>
</li>
<li>
<span>Inverse triangle inequality.</span>
</li>
<li>
<span>Weekly releases.</span>
</li>
</ul>
</section>
]]></content>
</entry>

<entry>
<title type="text">Zig defer Patterns</title>
<link href="https://matklad.github.io/2024/03/21/defer-patterns.html" rel="alternate" type="text/html" title="Zig defer Patterns" />
<published>2024-03-21T00:00:00+00:00</published>
<updated>2024-03-21T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/03/21/defer-patterns</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A short note about some unexpected usages of Zig's defer statement.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/03/21/defer-patterns.html"><![CDATA[
<h1><span>Zig defer Patterns</span> <time class="meta" datetime="2024-03-21">Mar 21, 2024</time></h1>
<p><span>A short note about some unexpected usages of Zig</span>&rsquo;<span>s </span><code>defer</code><span> statement.</span></p>
<p><span>This post assumes that you already know the basics about RAII, </span><code>defer</code><span> and </span><code>errdefer</code><span>. While</span>
<span>discussing the differences between them is not the point, I will allow myself one high level</span>
<span>comment. I don</span>&rsquo;<span>t like </span><code>defer</code><span> as a replacement for RAII: after writing Zig for some time, I am</span>
<span>relatively confident that humans are just not good at not forgetting defers, especially when</span>
&ldquo;<span>optional</span>&rdquo;<span> ownership transfer is at play (i.e, this function takes ownership of an argument, unless</span>
<span>an error is returned). But defer is good at discouraging RAII oriented programming. RAII encourages</span>
<span>binding lifetime of resources (such as memory) with lifetimes of individual domain objects (such as</span>
<span>a </span><code>String</code><span>). But often, in pursuit of performance and small code size, you want to separate the two</span>
<span>concerns, and let many domain objects to share the single pool of resources. Instead of each</span>
<span>individual string managing its own allocation, you might want to store the contents of all related</span>
<span>strings into a single continuously allocated buffer. Because RAII with defer is painful, Zig</span>
<span>naturally pushes you towards batching your resource acquisition and release calls, such that you have</span>
<span>far fewer resources than objects in your program.</span></p>
<p><span>But, as I</span>&rsquo;<span>ve said, this post isn</span>&rsquo;<span>t about all that. This post is about non-resource-oriented usages</span>
<span>of </span><code>defer</code><span>. There</span>&rsquo;<span>s more to defer than just RAII, it</span>&rsquo;<span>s a nice little powerful construct! This is way</span>
<span>to much ado already, so here come the patterns:</span></p>
<section id="Asserting-Post-Conditions">

    <h2>
    <a href="#Asserting-Post-Conditions"><span>Asserting Post Conditions</span> </a>
    </h2>
<p><code>defer</code><span> gives you poor man</span>&rsquo;<span>s contract programming in the form of</span></p>

<figure class="code-block">


<pre><code><span class="line">assert(precondition)</span>
<span class="line"><span class="hl-keyword">defer</span> assert(postcondition)</span></code></pre>

</figure>
<p><span>Real life </span><a href="https://github.com/tigerbeetle/tigerbeetle/blob/73bbc1a32ba2513e369764680350c099fe302285/src/vsr/grid.zig#L298-L309"><span>example</span></a><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">{</span>
<span class="line">  assert(<span class="hl-operator">!</span>grid.free_set.opened);</span>
<span class="line">  <span class="hl-keyword">defer</span> assert(grid.free_set.opened);</span>
<span class="line"></span>
<span class="line">  <span class="hl-comment">// Code to open the free set</span></span>
<span class="line">}</span></code></pre>

</figure>
</section>
<section id="Statically-Enforcing-Absence-of-Errors">

    <h2>
    <a href="#Statically-Enforcing-Absence-of-Errors"><span>Statically Enforcing Absence of Errors</span> </a>
    </h2>
<p><span>This is basically peak Zig:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">errdefer</span> <span class="hl-keyword">comptime</span> <span class="hl-keyword">unreachable</span></span></code></pre>

</figure>
<p><code>errdefer</code><span> runs when a function returns an error (e.g., when a </span><code>try</code><span> fails). </span><code>unreachable</code>
<span>crashes the program (in </span><code>ReleaseSafe</code><span>). But </span><code>comptime unreachable</code><span> straight up fails compilation</span>
<span>if the compiler tries to generate the corresponding runtime code. The three together ensure the</span>
<span>absence of error-returning paths.</span></p>
<p><span>Here</span>&rsquo;<span>s </span><a href="https://github.com/ziglang/zig/blob/1d82d7987acf7f020bcc6a976f9887a3556ef79c/lib/std/hash_map.zig#L1561-L1584"><span>an example</span></a>
<span>from the standard library, the function to grow a hash map:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment">// The function as a whole can fail...</span></span>
<span class="line"><span class="hl-keyword">fn</span><span class="hl-function"> grow</span>(</span>
<span class="line">  self: <span class="hl-operator">*</span>Self,</span>
<span class="line">  allocator: Allocator,</span>
<span class="line">  new_capacity: Size,</span>
<span class="line">) Allocator.Error<span class="hl-operator">!</span><span class="hl-type">void</span> {</span>
<span class="line">  <span class="hl-built_in">@setCold</span>(<span class="hl-literal">true</span>);</span>
<span class="line">  <span class="hl-keyword">var</span> map: Self = .{};</span>
<span class="line">  <span class="hl-keyword">try</span> map.allocate(allocator, new_capacity);</span>
<span class="line"></span>
<span class="line">  <span class="hl-comment">// ...but from this point on, failure is impossible</span></span>
<span class="line">  <span class="hl-keyword">errdefer</span> <span class="hl-keyword">comptime</span> <span class="hl-keyword">unreachable</span>;</span>
<span class="line"></span>
<span class="line">  <span class="hl-comment">// Code to rehash&amp;copy self to map</span></span>
<span class="line">  std.mem.swap(Self, self, <span class="hl-operator">&amp;</span>map);</span>
<span class="line">  map.deinit(allocator);</span>
<span class="line">}</span></code></pre>

</figure>
</section>
<section id="Logging-Errors">

    <h2>
    <a href="#Logging-Errors"><span>Logging Errors</span> </a>
    </h2>
<p><span>Zig</span>&rsquo;<span>s error handling mechanism provides only error code (a number) and an error trace. This is</span>
<span>usually plenty to programmatically handle the error in an application and for the operator to</span>
<span>debug a failure, but this is decidedly not enough to provide a nice report for the end user.</span>
<span>However, if you are in a business of reporting errors to users, you are likely writing an</span>
<span>application, and application might get away without propagating extra information about the error</span>
<span>to the caller. Often, there</span>&rsquo;<span>s enough context at the point where the error originates in the first</span>
<span>place to produce a user-facing report right there.</span></p>
<p><a href="https://github.com/tigerbeetle/tigerbeetle/blob/73bbc1a32ba2513e369764680350c099fe302285/src/tigerbeetle/benchmark_driver.zig#L158-L163"><span>Example:</span></a></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">const</span> port = port: {</span>
<span class="line">  <span class="hl-keyword">errdefer</span> <span class="hl-operator">|</span>err<span class="hl-operator">|</span> log.err(<span class="hl-string">&quot;failed to read the port number: {}&quot;</span>, .{err});</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">var</span> buf: [fmt.count(<span class="hl-string">&quot;{}<span class="hl-string">\n</span>&quot;</span>, .{maxInt(<span class="hl-type">u16</span>)})]<span class="hl-type">u8</span> = <span class="hl-literal">undefined</span>;</span>
<span class="line">  <span class="hl-keyword">const</span> len = <span class="hl-keyword">try</span> process.stdout.?.readAll(<span class="hl-operator">&amp;</span>buf);</span>
<span class="line">  <span class="hl-keyword">break</span> :port <span class="hl-keyword">try</span> fmt.parseInt(<span class="hl-type">u16</span>, buf[<span class="hl-numbers">0</span> .. len <span class="hl-operator">-</span><span class="hl-operator">|</span> <span class="hl-numbers">1</span>], <span class="hl-numbers">10</span>);</span>
<span class="line">};</span></code></pre>

</figure>
</section>
<section id="Post-Increment">

    <h2>
    <a href="#Post-Increment"><span>Post Increment</span> </a>
    </h2>
<p><span>Finally, </span><code>defer</code><span> can be used as an </span><code>i++</code><span> of sorts. </span><a href="https://github.com/tigerbeetle/tigerbeetle/blob/0.15.3/src/lsm/scan_buffer.zig#L97-L102"><span>For</span>
<span>example</span></a><span>,</span>
<span>here</span>&rsquo;<span>s how you can pop an item off a free list:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span><span class="hl-function"> acquire</span>(self: <span class="hl-operator">*</span>ScanBufferPool) Error<span class="hl-operator">!</span><span class="hl-operator">*</span><span class="hl-keyword">const</span> ScanBuffer {</span>
<span class="line">  <span class="hl-keyword">if</span> (self.scan_buffer_used <span class="hl-operator">==</span> constants.lsm_scans_max) {</span>
<span class="line">    <span class="hl-keyword">return</span> Error.ScansMaxExceeded;</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">defer</span> self.scan_buffer_used <span class="hl-operator">+=</span> <span class="hl-numbers">1</span>;</span>
<span class="line">  <span class="hl-keyword">return</span> <span class="hl-operator">&amp;</span>self.scan_buffers[self.scan_buffer_used];</span>
<span class="line">}</span></code></pre>

</figure>
</section>
]]></content>
</entry>

<entry>
<title type="text">Kafka versus Nabokov</title>
<link href="https://matklad.github.io/2024/03/02/Kafka-vs-Nabokov.html" rel="alternate" type="text/html" title="Kafka versus Nabokov" />
<published>2024-03-02T00:00:00+00:00</published>
<updated>2024-03-02T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/03/02/Kafka-vs-Nabokov</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Uplifting a lobste.rs comment to a stand-alone post.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/03/02/Kafka-vs-Nabokov.html"><![CDATA[
<h1><span>Kafka versus Nabokov</span> <time class="meta" datetime="2024-03-02">Mar 2, 2024</time></h1>
<p><span>Uplifting a lobste.rs comment to a stand-alone post.</span></p>
<p><code>objectif_lune</code><span> </span><a href="https://lobste.rs/s/9xtcun/complex_systems_bridging_between_spec"><span>asks</span></a><span>:</span></p>

<figure class="blockquote">
<blockquote><p><span>I am on the cusp (hopefully) of kicking off development of a fairly large and complex system</span>
<span>(multiple integrated services, kafkas involved, background processes, multiple client frontends,</span>
<span>etc…). It’s predominantly going to be built in rust (but that’s only trivially relevant; i.e. not</span>
<span>following standard OOP).</span></p>
<p><span>Here’s where i’m at:</span></p>
<ol>
<li>
<span>I have defined all the components, services, data stores to use / or develop</span>
</li>
<li>
<span>I have a a fairly concrete conceptualisation of how to structure and manage data on the storage</span>
<span>end of the system which i’m formalizing into a specification</span>
</li>
<li>
<span>I have a deployment model for the various parts of the system to go into production</span>
</li>
</ol>
<p><span>The problem is, I have a gap, from these specs of the individual components and services that need</span>
<span>to be built out, to the actual implementation of those services. I’ve scaffolded the code-base</span>
<span>around what “feels” like sensible semantics, but bridging from the scope, through the high-level</span>
<span>code organisation through to implementation is where I start to get a bit queasy.</span></p>
<p><span>In the past, i’ve more or less dove head-first into just starting to implement, but the problem has</span>
<span>been that I will very easily end up going in circles, or I end up with a lot of duplicated code</span>
<span>across areas and just generally feel like it’s not working out the way I had hoped (obviously</span>
<span>because i’ve just gone ahead and implemented).</span></p>
<p><span>What are some tools, processes, design concepts, thinking patterns that you can use to sort of fill</span>
<span>in that “last mile” from high-level spec to implementing to try and ensure that things stay on track</span>
<span>and limit abandonment or going down dead-ends?</span></p>
<p><span>I’m interested in advice, articles, books, or anything else that makes sense in the rough context</span>
<span>above. Not specifically around for instance design patterns themselves, i’m more than familiar with</span>
<span>the tools in that arsenal, but how do you bridge the gap between the concept and the implementation</span>
<span>without going too deep down the rabbit-hole of modelling out actual code and everything else in UML</span>
<span>for instance? How do you basically minimize getting mired in massive refactors once you get to</span>
<span>implementation phase?</span></p>
</blockquote>

</figure>
<p><span>My answer:</span></p>
<hr>
<p><span>I don’t have much experience building these kind of systems (I like Kafka, but I must say I prefer</span>
<span>Nabokov’s rendition of similar ideas in “Invitation to a Beheading” and “Pale Fire” more), but</span>
<span>here’s a couple of things that come to mind.</span></p>
<p><span>First, </span><a href="https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law"><span>every complex system that works started out as a simple system that worked</span></a><span>. Write code top</span>
<span>down: </span><a href="https://www.teamten.com/lawrence/programming/write-code-top-down.html" class="display url">https://www.teamten.com/lawrence/programming/write-code-top-down.html</a></p>
<p><span>Even if it is a gigantic complex system with many moving parts, start with spiking and end-to-end</span>
<span>solution which can handle one particular variation of a happy path. Build skeleton first, flesh can</span>
<span>be added incrementally.</span></p>
<p><span>To do this, you’ll need some way to actually run the entire system while it isn’t deployed yet,</span>
<span>which is something you need to solve before you start writing pages of code.</span></p>
<p><span>Similarly, include testing strategy in the specification, and start with one single simple</span>
<span>end-to-end test. I think that TDD as a way to design a class or a function is mostly snake oil</span>
<span>(because </span><a href="https://matklad.github.io/2021/05/31/how-to-test.html"><span>“unit” tests are mostly snake</span>
<span>oil</span></a><span>), but the overall large scale design of</span>
<span>the system should absolutely be driven by the way the system will be tested.</span></p>
<p><span>It is helpful to dwell on these two laws:</span></p>
<p><a href="https://martinfowler.com/articles/distributed-objects-microservices.html"><strong><strong><span>First Law of Distributed Object Design:</span></strong></strong></a></p>

<figure class="blockquote">
<blockquote><p><span>Don’t distribute your objects.</span></p>
</blockquote>

</figure>
<p><a href="https://en.wikipedia.org/wiki/Conway%27s_law"><strong><strong><span>Conway’s law:</span></strong></strong></a></p>

<figure class="blockquote">
<blockquote><p><span>Organizations which design systems are constrained to produce designs which are copies of the</span>
<span>communication structures of these organizations.</span></p>
</blockquote>

</figure>
<p><span>The code architecture of your solution is going to be isomorphic to your org chart, not to your</span>
<span>deployment topology. Let’s say you want to deploy three different services: </span><code>foo</code><span>, </span><code>bar</code><span>, and </span><code>baz</code><span>.</span>
<span>Just put all three into a single binary, which can be invoked as </span><code>app foo</code><span>, </span><code>app bar</code><span>, and </span><code>app
baz</code><span>. This mostly solves any code duplication issues — if there’s shared code, just call it!</span></p>
<p><span>Finally, system boundaries are the focus of the design:</span>
<a href="https://www.tedinski.com/2018/02/06/system-boundaries.html" class="display url">https://www.tedinski.com/2018/02/06/system-boundaries.html</a></p>
<p><span>Figure out hard system boundaries between “your system” and “not your system”, and do design those</span>
<span>carefully. Anything else that looks like a boundary isn’t. It is useful to spend some effort</span>
<span>designing those things as well, but it’s more important to make sure that you can easily change</span>
<span>them. Solid upgrade strategy for deployment trumps any design which seems perfect at a given moment</span>
<span>in time.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Window: Live, Constant Time Grep</title>
<link href="https://matklad.github.io/2024/02/10/window-live-constant-time-grep.html" rel="alternate" type="text/html" title="Window: Live, Constant Time Grep" />
<published>2024-02-10T00:00:00+00:00</published>
<updated>2024-02-10T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/02/10/window-live-constant-time-grep</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In this post, I describe the design of window --- a small
grep-like utility I implemented in 500 lines of Rust. The utility itself is likely not that
interesting --- I bet some greybeared can implement an equivalent in 5 lines of bash. But the
design principles behind it might be interesting --- this small utility manages to combine core
ideas of rust-analyzer and TigerBeetle!]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/02/10/window-live-constant-time-grep.html"><![CDATA[
<h1><span>Window: Live, Constant Time Grep</span> <time class="meta" datetime="2024-02-10">Feb 10, 2024</time></h1>
<p><span>In this post, I describe the design of </span><a href="https://github.com/matklad/window/"><span>window</span></a><span> </span>&mdash;<span> a small</span>
<span>grep-like utility I implemented in 500 lines of Rust. The utility itself is likely not that</span>
<span>interesting </span>&mdash;<span> I bet some greybeared can implement an equivalent in 5 lines of bash. But the</span>
<span>design principles behind it might be interesting </span>&mdash;<span> this small utility manages to combine core</span>
<span>ideas of rust-analyzer and TigerBeetle!</span></p>
<section id="Problem-Statement">

    <h2>
    <a href="#Problem-Statement"><span>Problem Statement</span> </a>
    </h2>
<p><span>TigerBeetle is tested primarily through a deterministic simulator: a cluster of replicas runs in a</span>
<span>single process (in a single thread even), replicas are connected to a virtual network and a virtual</span>
<span>hard drive. Both the net and the disk are extra nasty, and regularly drop, reorder, and corrupt IO</span>
<span>requests. The cluster has to correctly process randomly generated load in spite of this radioactive</span>
<span>environment. You can play with visualization of the simulator here:</span>
<a href="https://sim.tigerbeetle.com" class="display url">https://sim.tigerbeetle.com</a></p>
<p><span>Of course, sometimes we have bugs, and need to debug crashes found by the simulator. Because</span>
<span>everything is perfectly deterministic, a crash is a pair of commit hash and a seed for a random</span>
<span>number generator. We don</span>&rsquo;<span>t yet have any minimization infrastructure, so some crashes tend to be</span>
<span>rather large: a debug log from a crash can easily reach 50 gigabytes!</span></p>
<p><span>So that</span>&rsquo;<span>s my problem: given multi-gigabyte log of a crash, find a dozen or so of log-lines which</span>
<span>explain the crash.</span></p>
<p><span>I think you are supposed to use </span><code>coreutils</code><span> to solve this problem, but I am not good enough with</span>
<span>grep to make that efficient: my experience that grepping anything in this large file takes seconds,</span>
<span>and still produces gigabytes of output which is hard to make heads or tails of.</span></p>
<p><span>I had relatively more success with </span><a href="https://lnav.org"><span>lnav.org</span></a><span>, but:</span></p>
<ul>
<li>
<span>it is still slower than I would like,</span>
</li>
<li>
<span>it comes with its own unique TUI interface, shortcuts, and workflow, which is at odds with my</span>
<span>standard editing environment.</span>
</li>
</ul>
</section>
<section id="Window">

    <h2>
    <a href="#Window"><span>Window</span> </a>
    </h2>
<p><span>So, I made </span><code>window</code><span>. You run it as</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> window huge-file.log &amp;</span></code></pre>

</figure>
<p><span>It then creates two files:</span></p>
<ul>
<li>
<code>window.toml</code><span> </span>&mdash;<span> the file with the input query,</span>
</li>
<li>
<code>huge-file.log.window</code><span> </span>&mdash;<span> the result of the query.</span>
</li>
</ul>
<p><span>You open both files side-by-side in your editor of choice. Edits to the query file are immediately</span>
<span>reflected in the results file (assuming the editor has auto-save and automatically reloads files</span>
<span>changed on disk):</span></p>
<p><span>Here</span>&rsquo;<span>s a demo in Emacs (you might want to full-screen that video):</span></p>
<script async id="asciicast-637434" src="https://asciinema.org/a/637434.js"></script>
<p><span>In the demo, I have to manually save the </span><code>window.toml</code><span> file with </span><code>C-x C-s</code><span>, but in my</span>
<span>actual usage in VS Code the file is saved automatically after 100ms.</span></p>
<p><span>As you can see, </span><code>window</code><span> is pretty much instant. How is this possible?</span></p>
</section>
<section id="When-Best-Ideas-of-rust-analyzer-and-TigerBeetle-are-Combined-in-a-Tool-of-Questionable-Usefulness">

    <h2>
    <a href="#When-Best-Ideas-of-rust-analyzer-and-TigerBeetle-are-Combined-in-a-Tool-of-Questionable-Usefulness"><span>When Best Ideas of rust-analyzer and TigerBeetle are Combined in a Tool of Questionable</span>
<span>Usefulness</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s take a closer look at that query string:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-attr">reverse</span> = <span class="hl-literal">false</span></span>
<span class="line"><span class="hl-attr">position</span> = <span class="hl-string">&quot;0%&quot;</span></span>
<span class="line"><span class="hl-attr">anchor</span> = <span class="hl-string">&quot;&quot;</span></span>
<span class="line"><span class="hl-attr">source_bytes_max</span> = <span class="hl-number">104857600</span></span>
<span class="line"><span class="hl-attr">target_bytes_max</span> = <span class="hl-number">102400</span></span>
<span class="line"><span class="hl-attr">target_lines_max</span> = <span class="hl-number">50</span></span>
<span class="line"><span class="hl-attr">filter_in</span> = [</span>
<span class="line">      [<span class="hl-string">&quot;(replica): 0&quot;</span>, <span class="hl-string">&quot;view=74&quot;</span>],</span>
<span class="line">      [<span class="hl-string">&quot;(replica): 1&quot;</span>, <span class="hl-string">&quot;view=74&quot;</span>]</span>
<span class="line">]</span>
<span class="line"><span class="hl-attr">filter_out</span> = [</span>
<span class="line">       <span class="hl-string">&quot;ping&quot;</span>, <span class="hl-string">&quot;pong&quot;</span></span>
<span class="line">]</span></code></pre>

</figure>
<p><span>The secret sauce are </span><code>source_bytes_max</code><span> and </span><code>target_bytes_max</code><span> parameters.</span></p>
<p><span>Let</span>&rsquo;<span>s start with </span><code>target_bytes_max</code><span>. This is a lesson from </span><code>rust-analyzer</code><span>. For dev tools, the user</span>
<span>of software is a human. Humans are slow, and can</span>&rsquo;<span>t process a lot of information. That means it is</span>
<span>generally useless to produce more than a hundred lines of output </span>&mdash;<span> a human won</span>&rsquo;<span>t be able to make</span>
<span>use of a larger result set </span>&mdash;<span> they</span>&rsquo;<span>d rather refine the query than manually sift through pages of</span>
<span>results.</span></p>
<p><span>So, when designing software to execute a user-supplied query, the inner loop should have some idea</span>
<span>about the amount of results produced so far, and a short-circuiting logic. It is more valuable to</span>
<span>produce some result quickly and to inform the user that the query is not specific, than to spend a</span>
<span>second computing the full result set.</span></p>
<p><span>A similar assumption underpins the architecture of a lot of language servers. No matter the size of</span>
<span>the codebase, the amount of information displayed on the screen in user</span>&rsquo;<span>s IDE at a given point in</span>
<span>time is O(1). A typical successful language server tries hard to do the absolute minimal amount of</span>
<span>work to compute the relevant information, and nothing more.</span></p>
<p><span>So, the </span><code>window</code><span>, by default, limits the output size to the minimum of 100 kilobytes / 50 lines, and</span>
<span>never tries to compute more than that. If the first 50 lines of the output don</span>&rsquo;<span>t contain the result,</span>
<span>the user can make the query more specific by adding more AND terms to </span><code>filter_in</code><span> causes, or adding</span>
<span>OR terms to </span><code>filter_out</code><span>.</span></p>
<p><span>TigerBeetle gives </span><code>window</code><span> the second magic parameter </span>&mdash;<span> </span><code>source_bytes_max</code><span>. The big insight of</span>
<span>TigerBeetle is that all software always has limits. Sometimes the limit is a  hard wall: if a server</span>
<span>runs out of file descriptors, it just crashes. The limit can also be a soft, sloughy bog as well: if</span>
<span>the server runs out of memory, it might start paging memory in and out, slowing to a crawl. Even if</span>
<span>some requests are, functionally speaking, fulfilled, the results are useless, as they arrive too</span>
<span>late. Or, in other words, every request has a (potentially quite large) latency window.</span></p>
<p><span>It might be a good idea to make the limits explicit, and design software around them. That gives</span>
<span>predictable performance, and allows the user to manually chunk larger requests in manageable pieces.</span></p>
<p><span>That is exactly what </span><code>window</code><span> does. Grepping 100 megabytes is pretty fast. Grepping more might be</span>
<span>slow. So </span><code>window</code><span> just doesn</span>&rsquo;<span>t do it. Here</span>&rsquo;<span>s a rough rundown of the algorithm:</span></p>
<ol>
<li>
<code>mmap</code><span> the entire input file to a </span><code>&amp;[u8]</code><span>.</span>
</li>
<li>
<span>Wait until the control file (</span><code>window.toml</code><span>) changes and contains a valid query.</span>
</li>
<li>
<span>Convert the </span><code>position</code><span> field (which might be absolute or a percentage) to an absolute offset.</span>
</li>
<li>
<span>Select slice of </span><code>source_bytes_max</code><span> starting at that offset.</span>
</li>
<li>
<span>Adjust boundaries of the slice to be on </span><code>\n</code><span>.</span>
</li>
<li>
<span>Iterate lines.</span>
</li>
<li>
<span>If a line matches any of </span><code>filter_out</code><span> conditions, skip over it.</span>
</li>
<li>
<span>If a line matches any of </span><code>filter_in</code><span> conditions, add it to the result.</span>
</li>
<li>
<span>Break when reaching the end of </span><code>source_bytes_max</code><span> window, or when the size of output exceeds</span>
<code>target_bytes_max</code><span>.</span>
</li>
</ol>
<p><span>The deal is:</span></p>
<ul>
<li>
<span>It</span>&rsquo;<span>s on the user to position a limited window over the interesting part of the input.</span>
</li>
<li>
<span>In exchange, the </span><code>window</code><span> tool guarantees constant-time performance.</span>
</li>
</ul>
</section>
<section id="Limits-of-Applicability">

    <h2>
    <a href="#Limits-of-Applicability"><span>Limits of Applicability</span> </a>
    </h2>
<p><span>Important pre-requisites to make the </span>&ldquo;<span>limit the size of the output</span>&rdquo;<span> work are:</span></p>
<ul>
<li>
<span>The user can refine the query.</span>
</li>
<li>
<span>The results are computed instantly.</span>
</li>
</ul>
<p><span>If these assumptions are violated, it might be best to return the full list of results.</span></p>
<p><span>Here</span>&rsquo;<span>s one counterexample! I love reading blogs. When I find a great post, I often try to read all</span>
<span>other posts by the same author </span>&mdash;<span> older posts which are still relevant usually are much more</span>
<span>valuable then the news of the day. I love when blogs have a simple chronological list of all</span>
<span>articles, a-la: </span><a href="https://matklad.github.io" class="display url">https://matklad.github.io</a></p>
<p><span>Two blogging platforms mess up this feature:</span></p>
<p><span>WordPress blogs love to have </span>&ldquo;<span>archives</span>&rdquo;<span> organized by month, where a month</span>&rsquo;<span>s page typically has 1 to</span>
<span>3 entries. What</span>&rsquo;<span>s more, WordPress loves to display a couple of pages of content for each entry. This</span>
<span>is just comically unusable </span>&mdash;<span> the amount of </span><em><span>entries</span></em><span> on a page is too few to effectively search</span>
<span>them, but the actual amount of content on a page is overwhelming.</span></p>
<p><span>Substack</span>&rsquo;<span>s archive is an infinite scroll that fetches 12 entries at a time. 12 entries is a joke!</span>
<span>It</span>&rsquo;<span>s only 1kb compressed, and is clearly bellow human processing limit. There </span><em><span>might</span></em><span> be some</span>
<span>argument for client-side pagination to postpone loading of posts</span>&rsquo;<span> images, but feeding the posts</span>
<span>themselves over the network one tiny droplet at a time seems excessive.</span></p>
<hr>
<p><span>To recap:</span></p>
<ul>
<li>
<p><span>Limiting </span><em><span>output</span></em><span> size might be a good idea, because, with a human on the other side of display,</span>
<span>any additional line of output has a diminishing return (and might even be a net-negative). On the</span>
<span>other hand, constant-time output allows reducing latency, and can even push a batch workflow into</span>
<span>an interactive one</span></p>
</li>
<li>
<p><span>Limiting </span><em><span>input</span></em><span> size might be a good idea, because the input is </span><em><span>always</span></em><span> limited anyway. The</span>
<span>question is whether you know the limit, and whether the clients know how to cut their queries into</span>
<span>reasonably-sized batches.</span></p>
</li>
<li>
<p><span>If you have exactly the same 20 GB log file problems as me, you might install </span><code>window</code><span> with</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> cargo install --git https://github.com/matklad/window</span></code></pre>

</figure>
</li>
</ul>
</section>
]]></content>
</entry>

<entry>
<title type="text">Write Less</title>
<link href="https://matklad.github.io/2024/01/12/write-less.html" rel="alternate" type="text/html" title="Write Less" />
<published>2024-01-12T00:00:00+00:00</published>
<updated>2024-01-12T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/01/12/write-less</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[If we wish to count lines of code, we should not regard them as lines produced but as lines spent]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/01/12/write-less.html"><![CDATA[
<h1><span>Write Less</span> <time class="meta" datetime="2024-01-12">Jan 12, 2024</time></h1>

<figure class="blockquote">
<blockquote><p><span>If we wish to count lines of code, we should not regard them as </span>&ldquo;<span>lines produced</span>&rdquo;<span> but as </span>&ldquo;<span>lines spent</span>&rdquo;</p>
</blockquote>
<figcaption><cite><a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html"><span>Dijkstra</span></a></cite></figcaption>
</figure>
<p><span>The same applies to technical writing. There</span>&rsquo;<span>s a tendency to think that the more is written, the</span>
<span>better. It is wrong: given the same information content, a shorter piece of prose is easier to</span>
<span>understand, up to a reasonable limit.</span></p>
<p><span>To communicate effectively, write a bullet-point list of ideas that you need to get across. Then,</span>
<span>write a short paragraph in simple language that communicates these ideas precisely.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Of Rats and Ratchets</title>
<link href="https://matklad.github.io/2024/01/03/of-rats-and-ratchets.html" rel="alternate" type="text/html" title="Of Rats and Ratchets" />
<published>2024-01-03T00:00:00+00:00</published>
<updated>2024-01-03T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/01/03/of-rats-and-ratchets</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[This is going to be related to software engineering, pinky promise!]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/01/03/of-rats-and-ratchets.html"><![CDATA[
<h1><span>Of Rats and Ratchets</span> <time class="meta" datetime="2024-01-03">Jan 3, 2024</time></h1>
<p><span>This is going to be related to software engineering, pinky promise!</span></p>
<p><span>I was re-reading Doctor Zhivago by Boris Pasternak recently. It is a beautiful novel set in Russia</span>
<span>during the revolutionary years before World War II. It focuses on the life of Yuri Zhivago, a doctor</span>
<span>and a poet, while the Russian revolutions roar in the background. It is a poignant and topical tale</span>
<span>of a country descending into blood-thirsty madness.</span></p>
<p><span>Being a doctor, a literati, and a descendant of once wealthy family, Zhivago is not exactly welcomed</span>
<span>in the new Russia. That</span>&rsquo;<span>s why a significant part of the novel takes place far away from Moscow and</span>
<span>St. Petersburg, in Siberia, where it is easier for undesirables to exist in a fragile truce with the</span>
<span>state.</span></p>
<p><span>What</span>&rsquo;<span>s your first problem, if you are going to live in someone else</span>&rsquo;<span>s abandoned house in Siberia,</span>
<span>eking out a living off whatever supplies had been left? The rats, who are also very keen on the said</span>
<span>supplies. Clearly, rats are a big problem, and require immediate attention.</span></p>
<p><span>It</span>&rsquo;<span>s easy to exert effort and get rid of the rats </span>&mdash;<span> take a broom, some light source, and just</span>
<span>chase away the rascals from the house. However observably effective the method is, it is not a</span>
<span>solution </span>&mdash;<span> the rats will come back as soon as you are asleep. The proper solution starts with</span>
<span>identifying all the holes through which the pest gets in, and thoroughly plugging those! Only then</span>
<span>can you hope that the house </span><em><span>stays</span></em><span> rat free.</span></p>
<p><span>I feel the dynamics plays out in software projects. There</span>&rsquo;<span>s lots of rats, everything</span>&rsquo;<span>s broken and in</span>
<span>need of fixing, all the time. And there</span>&rsquo;<span>s usually plenty of desire and energy to fix things. The</span>
<span>problem is, often times the fixes are not durable </span>&mdash;<span> an immediate problem is resolved promptly, but</span>
<span>then it returns back two years down the line. This is most apparent in benchmarks </span>&mdash;<span> everyone loves</span>
<span>adding a microbenchmark to motivate a particular change, and then the benchmark bitrots with no one</span>
<span>to run it.</span></p>
<p><span>It</span>&rsquo;<span>s important not only to fix things, but to fix them in a durable way; to seal up the holes, not</span>
<span>just to wave the broom vigorously.</span></p>
<p><span>The best way to do this is to setup a not rocket science rule, and then to use it as a ratchet to</span>
<span>monotonically increase the set of properties the codebase possesses, one small check at a time.</span>
<span>Crucially, the ratchet should be set up up front, </span><em><span>before</span></em><span> any of the problems are actually fixed,</span>
<span>and it must allow for incremental steps.</span></p>
<p><span>Let</span>&rsquo;<span>s say you lack documentation, and want to ensure that every file in the code-base has a</span>
<span>top-level comment explaining  the relevant context. A good way to approach this problem is to write</span>
<span>a test that reads every file in the project, computes the set of poorly documented files, and xors</span>
<span>that against the hard-coded naughty list. This test is then committed to the project with the</span>
<span>naughty list encompassing all the existing files. Although no new docs are added, the ratchet is in</span>
<span>place </span>&mdash;<span> all new files are guaranteed to be documented. And its easier to move a notch up the</span>
<span>ratchet by documenting a single file and crossing it out from the naughty list.</span></p>
<p><span>More generally, widen your view of tests </span>&mdash;<span> a test is a program that checks a property of a</span>
<span>repository of code at a particular commit. Any property </span>&mdash;<span> code style, absence of warnings,</span>
<span>licenses of dependencies, the maximum size of any binary file committed into the repository,</span>
<span>presence of unwanted merge commits, average assertion density.</span></p>
<p><span>Not everything can be automated though. For things which can</span>&rsquo;<span>t be, the best trick I</span>&rsquo;<span>ve found is</span>
<span>writing them down. </span><em><span>Just</span></em><span> agreeing that </span><em><span>X</span></em><span> is a team practice is not enough, even if it </span><em><span>might</span></em>
<span>work for the first six months. Only when </span><em><span>X</span></em><span> is written down in a markdown document inside a</span>
<span>repository it might becomes a durable practice. But beware </span>&mdash;<span> document what </span><em><span>is</span></em><span>, rather than what</span>
<em><span>should</span></em><span> be. If there</span>&rsquo;<span>s a clear disagreement between what the docs say the world is, and the actual</span>
<span>world, the ratcheting effect of the written word disappears. If there</span>&rsquo;<span>s a large diff between reality</span>
<span>and documentation, don</span>&rsquo;<span>t hesitate to remove conflicting parts of the documentation. Having a ratchet</span>
<span>that enforces a tiny set of properties is much more valuable than aspirations to enforce everything.</span></p>
<p><span>Coming back to Doctor Zhivago, it is worth noting that the novel is arranged into a myriad of</span>
<span>self-contained small chapters </span>&mdash;<span> a blessing for a modern attention-deprived world, as it creates a</span>
<span>clear sense of progression even when you don</span>&rsquo;<span>t have enough focus to get lost in a book for hours.</span></p>
]]></content>
</entry>

</feed>
